\documentclass[article]{jss}
\usepackage{amsthm, amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator*{\argmax}{arg\,max}
%% almost as usual
\author{Hyungsuk Tak\\Harvard University \And 
             Joseph Kelly\\Google\And
             Carl Morris\\ Harvard University}
\title{\pkg{Rgbp}: An \proglang{R} Package for Conjugate Gaussian, Poisson, and Binomial Hierarchical Modeling and Frequency Method Checking on Overdispersed Data}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Hyungsuk Tak, Joseph Kelly, Carl Morris} %% comma-separated
\Plaintitle{Rgbp: Hierarchical Modeling and Frequency Method Checking} %% without formatting
\Shorttitle{\pkg{Rgbp}: Hierarchical Modeling and Frequency Method Checking on Overdispersed Data} %% a short title (if necessary)
%% an abstract and keywords
\Abstract{
\pkg{Rgbp} is an \proglang{R} package that utilizes  approximate Bayesian machinery to fit two-level conjugate hierarchical models on overdispersed Gaussian, Poisson, and Binomial data. The data that \pkg{Rgbp} assumes comprise of observed sufficient statistics for each random effect, such as averages or proportions, possibly together with covariates of each group but without population-level data. The approximate Bayesian tool equipped with the adjustment for density maximization produces point and interval estimates for each random effect, point estimates and standard errors for regression coefficients, and a point estimate and its standard error for a second-level variance component. For the Binomial data, the package provides an option to produce  posterior samples of all the model parameters via the acceptance-rejection method.  The main goal of \pkg{Rgb} is to produce approximate Bayesian interval estimates for the random effects that meet their nominal confidence levels, and the package uses unique improper hyper-prior distributions for that purpose.  \pkg{Rgbp} provides a quick way to check whether the resultant Bayesian interval estimates for the random effects achieve the nominal confidence levels via a repeated sampling coverage evaluation, which we call  ``frequency method checking."  }
% and evaluates repeated sampling frequency property of the resulting approximate Bayesian interval estimates for random effects
%It is found that such Bayesian-frequentist reconciliation allows \pkg{Rgbp} to have attributes desirable from both perspectives, working well in small samples and yielding good coverage probabilities for its interval estimates.
\Keywords{overdispersion, hierarchical model, adjustment for density maximization, repeated sampling coverage evaluation, \proglang{R}}
\Plainkeywords{multilevel model, conjugate hierarchical generalized linear models, frequency method checking, coverage probability, shrinkage, R} 
%% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Hyungsuk Tak\\
  Department of Statistics\\
  Harvard University\\
  1 Oxford Street, Cambridge, MA\\
  E-mail: \email{hyungsuk.tak@gmail.com}\\

  Joseph Kelly\\
  Google\\
  76 Ninth Avenue, New York, NY\\
  E-mail: \email{josephkelly@google.com}\\

  Carl Morris\\
  Department of Statistics\\
  Harvard University\\
  1 Oxford Street, Cambridge, MA\\
  E-mail: \email{morris@fas.harvard.edu}\\
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
Gaussian, Poisson, or Binomial data from several independent groups sometimes have more variation than the assumed Gaussian, Poisson, or Binomial distributions of the first-level observed data. To account for the extra-variability, called overdispersion, a  two-level conjugate hierarchical model regards first-level mean parameters as random effects that come from a population-level conjugate prior distribution. The conjugate prior distribution is non-exchangeable if the model incorporates covariate information of each group via a linear, log-linear, or logistic regression according to the data type, and exchangeable if no covariates are available with only an intercept term for the regression. 


With an assumption of homogeneity within each group, the observed data are sufficient statistics for the random effects, such as averages or proportions, possibly together with each group's covariate information. This type of  data is common for a biological analysis on litter data, a meta analysis on independent studies, or small area estimation problems.  For these data, the two-level model that \pkg{Rgbp} assumes can be considered as a conjugate hierarchical generalized linear model \citep{lee1996hierarchical, hglm2006} where each random effect has a conjugate prior distribution.


\pkg{Rgbp} takes a Bayesian approach with our special improper hyper-prior distributions on hyper-parameters, the parameters of the conjugate prior distribution, to produce  Bayesian interval   estimates for the random effects that achieve nominal confidence levels. The hyper-prior distributions lead to Stein's harmonic prior known to produce good repeated sampling coverage rates of the Bayesian interval estimates for random effects in a two-level Gaussian hierarchical model \citep{tang2011, morris2012, kelly2014advances}.  We apply an analog to Stein's harmonic prior to Poisson and Binomial hierarchical models. 
%The main goal of our Bayesian hierarchical models is to compare random effects between groups by their Bayesian interval estimates that have good frequency properties.


When it comes to fitting the model, \pkg{Rgbp} adopts the adjustment for density maximization \citep{carl1988, morris1997, tang2011} (ADM), a Pearson family approximation via maximization. For example, a Delta method is a special case of ADM that uses a Normal distribution to obtain an approximate distribution of a function of a parameter with its MLE and observed information plugged-in. In this article, the ADM uses Beta distributions to approximate the posterior distributions of shrinkage factors, functions of the second-level variance component, via maximization. For the approximation to the distributions of the shrinkage factors, the Beta distribution  is more appropriate than the Normal distribution (Delta method) because the support of each shrinkage factor is between 0 and 1, not ($-\infty,~\infty$). Also, the ADM is free of a boundary effect that a restricted MLE \citep{patterson1971recovery} (REML) sometimes suffers from, preventing a zero estimate for the second-level variance component. Using the approximate Beta posterior distributions of shrinkage factors, \pkg{Rgbp} estimates the first two (three for the Gaussian case) posterior moments of the random effects. Finally, \pkg{Rgbp} approximates the posterior distribution of each random effect by a skewed Normal distribution for a Gaussian case, a Gamma distribution for a Poisson case, and a Beta distribution for a Binomial case whose two parameters (three for the skewed Normal distribution) are matched to the previously estimated posterior moments of the random effects. 

For the Binomial  multilevel model, \pkg{Rgbp} provides an option to draw independent posterior samples of all the model parameters via an acceptance-rejection method instead of the approximate tool.


In addition to fitting hierarchical models, \pkg{Rgbp} provides a quick way to evaluate the repeated sampling coverage rates of the resulting Bayesian interval estimates for random effects \citep{morris1997, daniels1999prior, tang2002fitting, tang2011, morris2012}. It is a unique procedure that distinguishes \pkg{Rgbp}  from any other \proglang{R} packages for hierarchical modeling such as \pkg{hglm} \citep{hglm2010, ronnegaard2011hglm} for conjugate hierarchical generalized models and \pkg{arm} \citep{gelman2014arm} for Bayesian hierarchical regression models. The evaluation procedure which we call ``frequency method checking'' adopts a parametric bootstrapping that generates mock data sets given the values of the hyper-parameters and estimates the coverage rates based on the generated mock data sets.

The rest of this paper is organized as follows. We specify the Bayesian hierarchical models and discuss their posterior propriety in Section \ref{sec2}. We describe the estimation procedures including the ADM and the acceptance-rejection method in Section  \ref{sec3}, and the frequency method checking in Section \ref{sec4}.  We explain the usages of main functions in \pkg{Rgbp} in Section \ref{sec5}, and apply them to three examples in Section \ref{sec6}.



\section[Hierarchical Structure]{Conjugate hierarchical modeling structure} \label{sec2}
%A two-level  multilevel model is a powerful tool for exploring the hierarchical structure in data. For example, we can imagine that there exists a district-level hierarchy (bigger population) for observed school-level data, or a state-level hierarchy for observed hospital-level data in a certain state. 

One of the functions in \pkg{Rgbp}, \code{gbp}, fits a conjugate hierarchical model whose first-level has distributions of observed data and whose second-level has conjugate distributions on the first-level mean parameters (random effects). The \code{gbp} function allows users to choose one of three types of hierarchical models according to the type of data, namely Normal-Normal, Poisson-Gamma, and Binomial-Beta models. 
 
\subsection[Normal-Normal]{Normal-Normal (``g''=Gaussian data)}
The following is the general Normal-Normal hierarchical model assumed by \code{gbp}. For reference,  $V_{j}~(\equiv \sigma^{2}/n_{j})$ below is assumed to be known, and subscript \emph{j} indicates the \emph{j}-th group among \emph{k} groups in the dataset. For $j=1, 2, \ldots, k$, 
\begin{eqnarray}
y_{j}\vert \mu_{j} & \stackrel{indep.}{\sim} &\textrm{Normal}(\mu_{j}, V_{j}),\label{normalobs}\\
\mu_{j}\vert \boldsymbol{\beta}, A & \stackrel{indep.}{\sim} &\textrm{Normal}(\mu^E_{j}, A),\label{normalprior}
\end{eqnarray}


where $\mu^E_{j} = E(\mu_j\vert \boldsymbol{\beta}, A) = \boldsymbol{x}_j^\top \boldsymbol{\beta}=\beta_{1}x_{j, 1}+\beta_{2}x_{j, 2} + \cdots + \beta_{m}x_{j, m}$ is the expected random effect and $m$ is the number of regression coefficients to be estimated. The default of the function \code{gbp} is to set $x_{j, 1}$ to 1 for an intercept term, though \code{gbp} also provides a usage without the intercept term.  It is assumed that the second-level variance $A$ is unknown and that the $m\times1$ regression coefficient vector $\boldsymbol{\beta}$ is also unknown unless otherwise specified. If no covariates are available with an intercept term, then $\boldsymbol{x}_j^\top \boldsymbol{\beta}=\beta_1$ ($m=1$) and thus $\mu^E_{j}=\mu^E=\beta_1$ for all $j$, resulting in an exchangeable conjugate prior distribution for the random effects.  Based on these conjugate prior distributions for random effects, it is easy to derive the conditional posterior distributions of the random effects. For $j=1, 2, \ldots, k$,

\begin{equation} \label{normalpost}
\mu_{j}\vert  \boldsymbol{\beta}, A, \boldsymbol{y} \stackrel{indep.}{\sim}\textrm{Normal}(~(1-B_{j})y_{j} + B_{j}\mu^E_{j},~(1-B_{j})V_{j}~),
\end{equation}
where $B_{j}\equiv V_{j}/(V_{j} + A),~j=1, \ldots, k$, are called shrinkage factors, and $\boldsymbol{y}^T=(y_1, y_2, \ldots, y_k)$. Note that the conditional posterior mean of the random effect, $\mu^\ast_j\equiv E(\mu_{j}\vert \boldsymbol{\beta}, A, \boldsymbol{y} )=(1-B_{j})y_{j} + B_{j}\mu^E_{j}$,  is a convex combination of the observed sufficient statistic $y_j$ and the expected random effect $\mu^E_j$ weighted by the shrinkage factor $B_j$. If the variance of the conjugate prior distribution, $A$, is smaller than the variance of the observed distribution, $V_j$, then we expect the posterior mean to borrow more information from the more accurate second-level conjugate prior distribution.


\subsection[Poisson-Gamma]{Poisson-Gamma (``p''=Poisson data)}\label{poissonsubsec}
The function \code{gbp} is also capable of estimating a conjugate Poisson-Gamma hierarchical model, though its usage is limited to the case where the expected random effects are known ($m=0$).  For $j=1, 2, \ldots, k$, 
\begin{eqnarray}
y_j\vert \lambda_j &\stackrel{indep.}{\sim} & \textrm{Poisson}(n_{j}\lambda_{j}),\\
\lambda_{j}\vert r &\stackrel{i.i.d.}{\sim}& \textrm{Gamma}(r\lambda^E, r),
\end{eqnarray}
where $n_{j}$ is the exposure of group $j$, which is not necessarily an integer, $\lambda^E=E(\lambda_j\vert r)$ is the known expected random effect, and $r$ is the unknown second-level variance component. The mean and variance of the conjugate Gamma prior distribution are $\lambda^E$ and $\lambda^E/r$, respectively. We interpret $r$ as the amount of prior information as $n_{j}$ represents the amount of observed information, which makes intuitive sense because the uncertainty of the conjugate prior distribution increases as $r$ decreases; in the limit of $r$ going to 0, the conjugate prior distribution gets flatter. The conditional posterior distribution of the random effect $\lambda_j$ for this Poisson-Gamma model is
\begin{equation} \label{gammapost}
\lambda_j\vert r, \boldsymbol{y} \stackrel{indep.}{\sim}\textrm{Gamma}(r\lambda^E + n_j\bar{y}_{j},~ r + n_j),
\end{equation}  
whose $\bar{y}_j=y_j/n_j$. The mean and variance of the conditional posterior distribution are
\begin{equation}\label{gammapost_mean_var}
\lambda^\ast_j\equiv E(\lambda_j\vert r, \boldsymbol{y} )=(1-B_{j})\bar{y}_{j} + B_{j}\lambda^E~~\textrm{and}~~Var(\lambda_j\vert r, \boldsymbol{y})=\frac{\lambda^\ast_j}{r+n_j}.
\end{equation}
where $B_{j}\equiv r / (r+n_{j})$ for $j=1, 2, \ldots, k$.  The conditional posterior mean is  a convex combination of the observed sufficient statistic $\bar{y}_{j} = y_j/n_j$ and the expected random effect  $\lambda^E$ weighted by the relative amount of information in the prior compared to the data, called a shrinkage factor $B_j=r / (r + n_j)$. If the conjugate prior distribution contains more information than the observed data have, \emph{i.e.}, ensemble sample size $r$ exceeds individual sample size $n_{j}$, then the posterior mean shrinks  towards the prior mean by more than 50\%, borrowing more information from the prior distribution. 



Note that the conditional posterior variance in Equation \ref{gammapost_mean_var} is linear in the conditional posterior mean, whereas a slightly different Poisson-Gamma model specification has been used elsewhere \citep{morris1997} that makes the variances in the prior and conditional posterior distributions  quadratic functions of their means respectively.

\subsection[Binomial-Beta]{Binomial-Beta (``b''=Binomial data)}
The Binomial-Beta hierarchical model is the last model that \code{gbp} can fit. The notation $y_{j}$ is the number of successes (or failures) out of $n_{j}$ trials. Unlike the Poisson-Gamma model, the expected random effect is either known ($m=0$) or unknown ($m\ge1$) a priori.
\begin{eqnarray}
y_{j} \vert p_{j} &\stackrel{indep.}{\sim}& \textrm{Binomial}(n_{j}, ~p_{j}),\\
p_{j} \vert \boldsymbol{\beta}, r &\stackrel{indep.}{\sim}&\textrm{Beta}(rp^E_j,~ r(1-p^E_j)),
\end{eqnarray}

where $p^E_j\equiv E(p_j\vert \boldsymbol{\beta}, r)=\exp(\boldsymbol{x}_j^\top\boldsymbol{\beta})/(1+\exp(\boldsymbol{x}_j^\top\boldsymbol{\beta}))$ is the expected random effect  of group $j$ ($j=1, 2, \ldots, k$). The $m\times 1$ vector of the logistic regression coefficient $\boldsymbol{\beta}$ and the second-level variance component $r$ are unknown. The mean and variance of the conjugate Beta prior distribution for group $j$ are $p^E_j$ and $p^E_j(1-p^E_j)/(r+1)$, respectively.  The resultant conditional posterior distribution of random effect $j$  is
\begin{equation} \label{betapost}
p_{j}\vert  \boldsymbol{\beta}, r, \boldsymbol{y} \stackrel{indep.}{\sim}\textrm{Beta}(rp^E_j+n_{j}\bar{y}_{j},~r(1-p^E_j)+n_{j}(1-\bar{y}_{j})),
\end{equation}
whose $\bar{y}_j=y_j/n_j$ is the observed proportion of group $j$. The mean and variance of the conditional posterior distribution are
\begin{equation}\label{betapost_mean_var}
p_j^\ast\equiv E(p_j\vert \boldsymbol{\beta}, r, \boldsymbol{y} )=(1-B_{j})\bar{y}_{j} + B_{j}p^E_j~~\textrm{and}~~Var(p_j\vert  \boldsymbol{\beta}, r,  \boldsymbol{y})=\frac{p_j^\ast(1-p_j^\ast)}{r+n_j+1}.
\end{equation}
The conditional posterior mean is  a convex combination of the observed sufficient statistic $\bar{y}_{j} = y_j/n_j$ and the expected random effect  $p^E_j$ weighted by the relative amount of information in the prior compared to the data (a shrinkage factor) $B_j\equiv r / (r + n_j)$ like the Poisson-Gamma model.  If the conjugate prior distribution contains more information than the observed distribution does ($r>n_j$), then the resulting conditional posterior mean shrinks towards the expected random effect by more than 50\%.


\subsection[Hyper-prior distributions]{Hyper-prior Distribution}
Hyper-prior distributions are the distributions assigned to the second-level parameters called hyper-parameters. With the goal of objectivity in mind, our choice for the hyper-prior distributions is
\begin{equation}\label{eq:hyper}
\boldsymbol{\beta} \sim \textrm{Uniform on}~ \textbf{R}^{m}~~\textrm{and}~~A \sim \textrm{Uniform}(0, \infty) ~~(\textrm{or} ~\frac{1}{r}\sim \textrm{Uniform}(0, \infty)),
\end{equation}

where $m$ is the number of the regression coefficients to be estimated. The improper flat hyper-prior distribution on $\boldsymbol{\beta}$ is a common non-informative choice.  In the Gaussian case, the flat hyper-prior distribution on the second-level variance $A$ produces good repeated sampling coverage properties of the Bayesian interval estimates for the random effects. The resulting full posterior distribution of the random effects and hyper-parameters is proper if $k\ge m+3$ \citep{tang2011, kelly2014advances}. 
% The improper uniform distribution on the second-level variance component over the positive real line is to ensure good frequency properties.


In the other two cases, Poisson-Gamma and Binomial-Beta, the flat prior distribution on $1/r$ induces the same improper prior distribution on shrinkages ($\pi(B_{j})\propto B_{j}^{-2} d B_j$) as does $A$ with Uniform($0, \infty$) for the Gaussian case. The resultant full posterior distribution of random effects and hyper-parameters for the Binomial case is data-dependent. Let's define an ``interior group'' as the group whose number of successes $y_j$ are neither 0 nor $n_j$ and $k_y$ as the number of interior groups among the entire $k$ groups. Then, the full posterior distribution of random effects and hyper-parameters is proper if and only if there are at least two interior groups in the data and the $k_y\times m$ covariate matrix of the interior groups is of full rank $m$ \citep{tak2016propriety}. 


The Poisson-Gamma model with the hyper-prior distributions in Equation \ref{eq:hyper} provides posterior propriety if and only if there are at least two groups whose observed values $y_j$ are non-zero  and the expected random effect $\lambda^E$ is a completely known constant ($m=0$). If the expected random effect is unknown a priori,  then we recommend staying with the Binomial-Beta model with the same hyper-prior distributions because the Poisson-Gamma model is actually an approximation to the Binomial-Beta model. 



\section[Estimation]{Inference via the adjustment for density maximization}\label{sec3}

Obtaining point and interval estimates of each random effect is our primary inferential interest.  The approximate Bayesian tool assumes that the unconditional posterior distribution of each random effect follows a skewed-Normal distribution for the Gaussian case,  a Gamma distribution for the Poisson case, or a Beta distribution for the Binomial case whose parameters are matched to the estimated unconditional posterior moments of each random effect. We use these assumed unconditional posterior distributions to make point and interval estimates of each random effect. 

We illustrate our estimation procedure equipped with the adjustment for density maximization (hereafter ADM) \citep{carl1988, morris1997, tang2011}, a way to  approximate a distribution of the parameter of interest by one of Pearson family distributions based on derivatives like the Delta method, using the Binomial and Poisson hierarchical models. The ADM procedure for the Gaussian hierarchical model adopted in  \pkg{Rgbp} has been well-documented in \cite{kelly2014advances}.

\subsection[Inference]{The inferential model}\label{inference}


The likelihood function of hyper-parameters $r$ ($A$ for the Gaussian model) and $\boldsymbol{\beta}$ for the Binomial hierarchical model is derived from the independent Beta-Binomial marginal distributions of the observed data with random effects integrated out \citep{skellam1948}. %For reference, the marginal distribution of the observed datum for the Normal model  still follows a Normal distribution and that for the Poisson model follows a Negative-Binomial distribution.
\begin{equation}\label{marginal_binomial}
L(r, \boldsymbol{\boldsymbol{\beta}})=\prod_{j=1}^k f(y_j\vert r, \boldsymbol{\boldsymbol{\beta}})=\prod_{j=1}^k\binom{n_j}{y_j}\frac{B(y_j+rp^E_j, ~n_j-y_j+r(1-p^E_j))}{B(rp^E_j, ~r(1-p^E_j))},
\end{equation}
where the notation $B(a, b)~(\equiv\int_0^1 v^{a-1}(1-v)^{b-1}dv)$ indicates a beta function for positive constants $a$ and $b$. Similarly, the likelihood function of $r$ for the Poisson hierarchical model comes from the independent Negative-Binomial marginal distributions of the observed data with the random effects integrated out;
\begin{equation}\label{marginal_poisson}
L(r)=\prod_{j=1}^k f(y_j\vert r)=\prod^{k}_{j=1} \frac{\Gamma(r \lambda^E+y_j)}{\Gamma(r\lambda^E)(y_j!)}(1-B_{j})^{y_{i}}B_{j}^{r \lambda^E}.
\end{equation}


%If all the observed data are extreme, i.e., $y_j$ is either 0 or $n_j$ for all $j$, the likelihood becomes a decreasing function with respect to $r$ for finite values of $\boldsymbol{\beta}$. In this case, the MLE of $r$ is 0.

The joint posterior density  function of hyper-parameters $f(r, \boldsymbol{\boldsymbol{\beta}}\vert \boldsymbol{y})$ for the Binomial hierarchical model is proportional to their likelihood functions in Equation \ref{marginal_binomial} multiplied by the hyper-prior density functions of $r$ and $\boldsymbol{\beta}$ in Equation \ref{eq:hyper} as follows;
\begin{equation}\label{marginal_post}
f(r, \boldsymbol{\boldsymbol{\beta}}\vert \boldsymbol{y})\propto L(r, \boldsymbol{\boldsymbol{\beta}})d\boldsymbol{\beta}dr/r^2.
\end{equation}
The posterior density function of $r$, $f(r\vert \boldsymbol{y})$, for the Poisson hierarchical model is  the likelihood function in Equation \ref{marginal_poisson} times the hyper-prior density function of $r$, $dr/r^2$;
\begin{equation}\label{marginal_post}
f(r \vert \boldsymbol{y})\propto L(r)dr/r^2.
\end{equation}

Our goal is to obtain the point and interval estimates of the random effects from their unconditional posterior distributions; for the Binomial case, 
\begin{equation}\label{mcintegration}
f(\boldsymbol{p}\vert \boldsymbol{y})=\int  f(\boldsymbol{p}\vert r, \boldsymbol{\beta}, \boldsymbol{y})\cdot f(r, \boldsymbol{\beta}\vert \boldsymbol{y})dr d\boldsymbol{\beta},
\end{equation}
and for the Poisson case, 
\begin{equation}\label{mcintegration}
f(\boldsymbol{\lambda}\vert \boldsymbol{y})=\int  f(\boldsymbol{\lambda}\vert r, \boldsymbol{y})\cdot f(r\vert \boldsymbol{y})dr.
\end{equation}

\subsection[shrinkage]{Estimation for shrinkage factors and expected random effects}\label{shrinkage}

Estimating the unconditional posterior moments of the shrinkage factors, $B_1, B_2, \ldots, B_k\equiv r/(r+n_k)$, and the conditional posterior moments of the expected random effects, $p^E_1, p^E_2, \ldots, p^E_k$ (or $\lambda^E$ for the Poisson model), is the most important estimation problem for the hierarchical models that \code{gbp} assumes. This is because these moment estimates are used to approximate the unconditional posterior moments of the random effects, $p_1, p_2, \ldots, p_k$ (or $\lambda_1, \lambda_2, \ldots, \lambda_k$ for the Poisson model). Taking the Binomial-Beta model as an example, with the assumption that hyper-parameters $r$ (or $A$ for the Gaussian model) and $\boldsymbol{\beta}$ are independent a posteriori, the unconditional posterior mean and variance of random effect $j$ are
\begin{eqnarray}
E(p_{j}\vert \boldsymbol{y}) &=&E(E(p_{j}\vert r, \boldsymbol{\beta}, \boldsymbol{y})\vert\boldsymbol{y})=(1-E(B_j\vert\boldsymbol{y}))\bar{y}_j + E(B_j\vert  \boldsymbol{y})E(p^E_{j}\vert  \boldsymbol{y})\label{postmean} \\
~~~~~~Var(p_{j}\vert \boldsymbol{y}) &=&  E(Var(p_{j}\vert r, \boldsymbol{\beta}, \boldsymbol{y})\vert \boldsymbol{y})+Var(E(p_{j}\vert r, \boldsymbol{\beta},\boldsymbol{y})\vert \boldsymbol{y})\label{postvar}\\
&=& E\big(p^{\ast}_{j}(1-p^{\ast}_{j})/ (r+n_{j}+1)\vert  \boldsymbol{y}\big)+Var\big(\textrm{B}_{j}(\bar{y}_{j}-p^E_{j})\vert \boldsymbol{y}\big) \\
&\approx&E\big(p^{\ast}_{j}(1-p^{\ast}_{j})(1-\textrm{B}_{j})/n_{i}\vert \boldsymbol{y}\big)+Var\big(\textrm{B}_{j}(\bar{y}_{j}-p^E_{j})\vert \boldsymbol{y}\big)\\
&=&g(E(B_j\vert\boldsymbol{y}), E(B^2_j\vert\boldsymbol{y}), E(B^3_j\vert\boldsymbol{y}),  E(p^E_{j}\vert\boldsymbol{y}), E((p^E_{j})^2\vert\boldsymbol{y})).\label{approximate_var}
\end{eqnarray}
Note that the unconditional posterior mean and approximate variance of random effect $j$ in Equation \ref{postmean}  and \ref{approximate_var}  are functions of the unconditional posterior moments of shrinkage factors and expected random effects. We specify the function $g$ in Equation \ref{approximate_var} in Appendix \ref{apppostvar}.  

We assumed that hyper-parameters $r$ and  $\boldsymbol{\beta}$ were independent a posteriori, considering that they are independent a posteriori in the limit of $k$ going to infinity  because they are asymptotically Normally distributed. Also, \cite{morris1997}  empirically showed that their covariance from the observed information matrix of the Poisson-Gamma model, though with a different parametrization, was close to 0 in a small sample setting.

For the Poisson model, the unconditional posterior mean and variance of random effect $j$ are
\begin{eqnarray}
E(\lambda_j\vert \boldsymbol{y}) &=&E(E(p_{j}\vert r, \boldsymbol{y})\vert\boldsymbol{y})=(1-E(B_j\vert\boldsymbol{y}))\bar{y}_j + E(B_j\vert  \boldsymbol{y})\lambda^E\label{postmean_poisson} \\
~~~~~~Var(\lambda_{j}\vert \boldsymbol{y}) &=&  E(Var(p_{j}\vert r,  \boldsymbol{y})\vert \boldsymbol{y})+Var(E(\lambda_{j}\vert r, \boldsymbol{y})\vert \boldsymbol{y})\label{postvar_poisson}\\
&=& E\big(\lambda^{\ast}_{j}/ (r+n_{j})\vert  \boldsymbol{y}\big)+Var\big(\textrm{B}_{j}(\bar{y}_{j}-\lambda^E_{j})\vert \boldsymbol{y}\big) \\
&=&h(E(B_j\vert\boldsymbol{y}), E(B^2_j\vert\boldsymbol{y}) ).\label{approximate_var_poisson}
\end{eqnarray}
The unconditional posterior mean and variance of random effect $j$ under the Poisson model are also functions of the unconditional posterior moments of the shrinkage factors. We specify the function $h$ in Equation \ref{approximate_var_poisson} in Appendix \ref{apppostvar_poisson}.
%conditional posterior means of  random effect $j$ in Equation  \ref{normalpost}, \ref{gammapost_mean_var}, and \ref{betapost_mean_var} are a linear function of the shrinkage factor $B_{j}$ and the conditional posterior variances in Equation  \ref{normalpost}, \ref{gammapost_mean_var}, and \ref{betapost_mean_var} are linear (Gaussian), quadratic (Poisson), and  approximately cubic (Binomial) functions of $B_{j}$, considering $r+n_j=n_j/(1-B_j)$.  Thus, the unconditional posterior mean and variance of the random effect are the functions of the unconditional posterior moments of the shrinkage factors. 

Next, we estimate the unconditional posterior moments of the shrinkage factors and expected random effects after approximating their unconditional posterior distributions by Beta distributions via the ADM. 

\subsubsection{Unconditional posterior moments of shrinkage factors.}  It is noted that  the shrinkage factors ($B_1, \ldots B_k$) are a function of $r$, i.e., $B_{j}= r/(r+n_{j})=B_{j}(r)$. One way to approximate the distribution of $B_{j}$ is to find the maximum likelihood estimate of $r$, $\hat{r}_{MLE}$, with its Hessian value and to use a Delta method for an asymptotic Normal distribution of $B_{j}(\hat{r}_{MLE})$. This Normal approximation, however, is defined on $(-\infty, \infty)$ whereas $B_{j}$ lies on the unit interval between 0 and 1, and hence in small sample sizes this approximation can be quite flawed and can even result in point estimates lying on the boundary of the parameter space, from which the restricted MLE procedure sometimes suffers \citep{tang2011, kelly2014advances}.

To continue with a maximization-based estimation procedure but to steer clear of aforementioned boundary issues we make use of the ADM \citep{carl1988, morris1997, tang2011}. The ADM approximates the distribution of the function of the parameter of interest by one of the Pearson family distributions using the first two derivatives as the Delta method does; the Delta method is a special case of the ADM based on the Normal distribution. For our purposes we approximate the posterior distribution of a shrinkage factor with a Beta distribution, which allows us to finally obtain estimates of the posterior moments, i.e.,  of $E(B_{j}^c\vert\boldsymbol{y})$ for $c\ge0$.  \citet{morris1997} showed that the Beta approximation for the shrinkage factors worked better than Normal approximation using the Poisson hierarchical model when the sample size was small, and \citet{tang2011} showed that  using the Normal hierarchical model. 

The ADM assumes that the shrinkage factors follow Beta distributions a posteriori as
\begin{equation}\label{admshrinkage}
B_j\vert\boldsymbol{y}\sim Beta(a_{1j}, a_{0j}), ~\textrm{for}~j=1, 2, \ldots, k,
\end{equation}
and the ADM estimates the two parameters of the Beta distribution, i.e., $a_{1j}$ and $a_{0j}$.
%The estimated Beta distribution with the estimates $\hat{a}_{1j}$ and $\hat{a}_{0j}$ will be used to approximate the posterior moments of the shrinkage factors. 
% and $h(B_j)=\log(H(B_j))$Hessian
%whose mean and variance are estimated via two derivatives of an adjusted posterior density and %matched to the two parameters of the Beta distribution, $a_{1j}$ and $a_{0j}$. 


Note that the mean of Beta distribution $a_{1j}/(a_{1j}+a_{0j})$ is not the same as its mode $(a_{j1}-1)/(a_{j1}+a_{j0}-2)$. ADM works on an adjusted posterior distribution $A(B_j\vert \boldsymbol{y})dB_j\propto B_j(1-B_j)f(B_j\vert \boldsymbol{y})dB_j$ so that its mode is the same as the mean of the original Beta distribution. The assumed posterior mean and variance of shrinkage factor are
\begin{eqnarray}
E(B_j\vert\boldsymbol{y})&=&\frac{a_{1j}}{a_{1j}+a_{0j}}=\argmax_{B_j}~ A(B_j\vert \boldsymbol{y})\equiv B^\ast_j,\label{admmean}\\
Var(B_j\vert\boldsymbol{y})&=&\frac{B^\ast_j(1-B^\ast_j)}{a_{1j}+a_{0j}+1}=\frac{B^\ast_j(1-B^\ast_j)}{B^\ast_j(1-B^\ast_j)[-\frac{d^2}{dB^2_j}\log(A(B_j\vert \boldsymbol{y}))\vert_{B_j=B^\ast_j}] +1}.\label{admvar}
\end{eqnarray}

ADM estimates these mean and variance using the marginal posterior distribution of $r$, $f(r\vert\boldsymbol{y})\propto L(r)dr/r^2$, where the marginal likelihood $L(r)=\int L(\boldsymbol{\beta}, r)d\boldsymbol{\beta}$ for the Binomial model is obtained via Laplace approximation with a Lebesque measure on $\boldsymbol{\beta}$ and that for the Poisson model is specified in Equation \ref{marginal_poisson}; see \cite{berger1999integrated} for the integrated likelihood in detail. Considering that Equation \ref{admmean} and  \ref{admvar} involve the maximization and Hessian calculation, we work on a logarithmic scale of $r$, i.e., $\alpha=-log(r)$, because the distribution of $\alpha$ is more symmetric than that of $r$ and $\alpha$ is defined on a real line without any boundary issues. Since $A(B_j\vert\boldsymbol{y})$ is proportional to the marginal posterior density $f(\alpha\vert\boldsymbol{y})\propto e^\alpha L(\alpha)$, the estimated posterior mean in Equation \ref{admmean} is 
\begin{equation}\label{meaninvariance}
\hat{B}^\ast_j=\frac{e^{-\hat{\alpha}}}{n_j+e^{-\hat{\alpha}}},
\end{equation}

%This is because the marginal posterior distribution of the shrinkage factor, $p(B_j\vert\boldsymbol{y})dB_j \propto L(B_j)dB_j/B^2_j$ can be obtained via its variable transformation.

in which $\hat{\alpha}$ is the mode of $f(\alpha\vert\boldsymbol{y})$, \emph{i.e.}, $\argmax_{\alpha}\{\alpha+\log(L(\alpha))\}$ .
%Next, a variable transformation gives us .

We need the invariance information \citep{tang2011} to estimate the variance in Equation \ref{admvar}, which is defined as 
\begin{eqnarray}
\textrm{inv.info} &\equiv& -\frac{d^2 \log(A(B_j\vert \boldsymbol{y}))}{d[logit(B_j)]^2}\bigg\vert_{B_j=\hat{B}^\ast_j}=-\frac{d^2 \log(A(B_j(r)\vert \boldsymbol{y}))}{d[\log(r)]^2}\bigg\vert_{r=\hat{r}}\label{invariance}\\
&=& -\frac{d^2 \log(A(B_j(r(\alpha))\vert \boldsymbol{y}))}{d\alpha^2}\bigg\vert_{\alpha=\hat{\alpha}}\nonumber
\end{eqnarray}

Note that this invariance information is  the negative Hessian value of $\alpha+\log(L(\alpha))$ at the mode $\hat{\alpha}$. Using the invariance information, we  estimate the posterior variance in Equation \ref{admvar} as
\begin{equation}\label{varianceinvariance}
\widehat{Var}(B_j\vert\boldsymbol{y})=\frac{\hat{B}^{\ast2}_j(1-\hat{B}^\ast_j)^2}{inv.info +\hat{B}^\ast_j(1-\hat{B}^\ast_j)}.
\end{equation}

After matching the estimated unconditional posterior mean and variance of shrinkage factor $j$ in Equation \ref{meaninvariance} and \ref{varianceinvariance} to the two parameters of the Beta distribution in Equation \ref{admshrinkage}, i.e., $a_{1j}$ and $a_{0j}$, we get their estimates as
\begin{equation}\label{admpara}
\hat{a}_{1j}=\frac{\textrm{inv.info}}{1-\hat{B}^\ast_j}~~\textrm{and}~~\hat{a}_{0j}=\frac{\textrm{inv.info}}{\hat{B}^\ast_j}.
\end{equation}

The moments of the Beta distribution are well defined as a function of $a_{1j}$ and $a_{0j}$; $E(B^c_j\vert\boldsymbol{y})=B(a_{1j} + c, a_{0j})/B(a_{1j}, a_{0j})$ for $c\ge0$. Their estimates are 
\begin{equation}\label{shrinkagemoments}
\widehat{E}(B^c_j\vert\boldsymbol{y})= \frac{B(\hat{a}_{1j} + c, \hat{a}_{0j})}{B(\hat{a}_{1j}, \hat{a}_{0j})},
\end{equation}

and the approximation gets better if the true posterior distribution of the shrinkage factor is closer to the Beta distribution \citep{morris1997, tang2011, morris2012}.




\subsubsection{Unconditional posterior moments of expected random effects for the Binomial model.}
We assume that the conditional posterior distribution of each expected random effect given $\hat{\alpha}$ follows a Beta distribution, and estimate the assumed Beta distribution using the first and second derivatives of the conditional posterior distribution of $\beta$ given $\hat{\alpha}$. We estimate the conditional posterior moments of the expected random effects and treat them as their estimated unconditional posterior moments obtained by the (Laplace) approximate marginal likelihood function of $\boldsymbol{\beta}$. We found that the conditional and unconditional posterior moment estimates were almost identical due to their small covariance between $r$ and $\boldsymbol{\beta}$ in the observed information matrix. In addition, calculating the conditional posterior moments was computationally less burdensome. 

The moments of the expected random effects ($p^E_{1}, p^E_{2}, \ldots p^E_{k}$) involve an intractable integration. For example, the first conditional posterior moment is
\begin{equation}\label{priormoment1}
E(p^E_{j}\vert\hat{\alpha}, \boldsymbol{y})=E\bigg(\frac{e^{x_j^\top\boldsymbol{\beta}}}{1+e^{x_j^\top\boldsymbol{\beta}}}\bigg\vert\hat{\alpha}, \boldsymbol{y}\bigg)=\int_{\mathbf{R}^{m}} \frac{e^{x_j^\top\boldsymbol{\beta}}}{1+e^{x_j^\top\boldsymbol{\beta}}}f(\beta\vert\hat{\alpha}, \boldsymbol{y})d\boldsymbol{\beta}.
\end{equation}

Considering that the expected random effects are a function of logistic regression coefficients $\boldsymbol{\beta}$, we can use the Delta method for the asymptotic Normal distribution of $p^E_{j}(\hat{\boldsymbol{\beta}}_{MLE})$. However, the Normal approximation goes through the same support and boundary issues as the shrinkage factors do. Instead, we approximate the conditional posterior distribution of $p^E_{j}(\boldsymbol{\beta})$ by a Beta distribution using $\hat{\beta}_{MLE}$ and the Hessian value at the MLE.

We assume the conditional posterior distribution of expected random effect $j$ is a Beta distribution as follows;
\begin{equation}\label{admpriormean}
p^E_{j}\vert\hat{\alpha}, \boldsymbol{y}=\frac{e^{x_j^\top\boldsymbol{\beta}}}{1+e^{x_j^\top\boldsymbol{\beta}}}\bigg\vert\hat{\alpha}, \boldsymbol{y}\sim \textrm{Beta}(b_{1j}, b_{0j})\sim \frac{G(b_{1j})}{G(b_{1j})+G(b_{0j})},
\end{equation}

where $G(b_{1j})$ is a random variable following a Gamma$(b_{1j}, 1)$ distribution with a unit scale and independently $G(b_{0j})$ has a Gamma($b_{0j}, 1)$ distribution.  Note that the representation in Equation (\ref{admpriormean}) is equivalent to saying $e^{x_j^\top\boldsymbol{\beta}}\vert\hat{\alpha}, \boldsymbol{y}\sim G(b_{1j})/G(b_{0j})$, a ratio of two independent Gamma random variables. Its mean and variance are
\begin{eqnarray}
E(e^{x_j^\top\boldsymbol{\beta}}\vert\hat{\alpha}, \boldsymbol{y})&=&E\bigg(\frac{G(b_{1j})}{G(b_{0j})}\bigg)=\frac{b_{1j}}{b_{0j}-1}\equiv\eta_j,\label{priormean1}\\
~~~~~Var(e^{x_j^\top\boldsymbol{\beta}}\vert\hat{\alpha}, \boldsymbol{y})&=&Var\bigg(\frac{G(b_{1j})}{G(b_{0j})}\bigg)=\frac{\eta_j(1+\eta_j)}{b_{0j}-2}.\label{priorvariance1}
\end{eqnarray}

In order to estimate $b_{1j}$ and $b_{0j}$, we assume that the conditional posterior distribution of $\beta$ follows $N[\hat{\beta}, \hat{\Sigma}]$, where $\hat{\boldsymbol{\beta}}$ is the mode of $p(\boldsymbol{\beta}\vert \hat{\alpha}, \boldsymbol{y})$  and $\hat{\Sigma}$ is a negative Hessian matrix at the mode. Then the posterior distribution of $x_j^\top\boldsymbol{\beta}$ is also Normal with mean $x_j^\top\hat{\boldsymbol{\beta}}$ and variance $x_j^\top\hat{\Sigma} x_j$.

Using the property of the log-Normal distribution, we get the estimates of posterior mean and variance in Equation (\ref{priormean1}) and (\ref{priorvariance1}) as
\begin{eqnarray}
\hat{\eta}_j&=&e^{x_j^\top\hat{\boldsymbol{\beta}}+x_j^\top\hat{\Sigma} x_j/2},\label{priormean2}\\
~~~~~\widehat{Var}(e^{x_j^\top\boldsymbol{\beta}}\vert\boldsymbol{y})&=&\hat{\eta}^2_j\big(e^{x_j^{T}\hat{\Sigma} x_j}-1\big).\label{priorvariance2}
\end{eqnarray}

By matching the estimated mean and variance in Equation (\ref{priormean2}) and (\ref{priorvariance2}) to $b_{1j}$ and $b_{0j}$ in Equation (\ref{priormean1}) and (\ref{priorvariance1}), we obtain the estimates of $b_{1j}$ and $b_{0j}$ as follows;
\begin{equation}\label{priormeanpara}
\hat{b}_{1j}=\hat{\eta}_j+\frac{\hat{\eta}_j + 1}{e^{x_j^{T}\hat{\Sigma} x_j}-1}~~\textrm{and}~~\hat{b}_{0j}=\frac{\hat{\eta}_j + 1}{\hat{\eta}_j(e^{x_j^{T}\hat{\Sigma} x_j}-1)}+2.
\end{equation}

The conditional posterior moments of expected random effects can be estimated similarly to those of shrinkage factors;  $E((p^E_{j})^d\vert\hat{\alpha}, \boldsymbol{y})=B(b_{1j} + d, b_{0j})/B(b_{1j}, b_{0j})$ for $d\ge0$. Finally, we estimate the unconditional posterior moments of expected random effects by their estimated conditional posterior moments as follows.
\begin{equation}\label{priormeanmoments}
\widehat{E}((p^E_{j})^d\vert\boldsymbol{y})= \frac{B(\hat{b}_{1j} + d, \hat{b}_{0j})}{B(\hat{b}_{1j}, \hat{b}_{0j})}.
\end{equation}

%Considering the histogram in the middle of Fig \ref{fig2},  we can see that the ADM approximation (density line) to 20,000 posterior samples of $p_{01}$ obtained via grid-sampling based on 18 baseball data look satisfactory.


\subsection{Estimation for random effects}
It is intractable to derive the unconditional posterior distribution of each random effect analytically for both Binomial and Poisson models. Instead, we again assume that each random effect of the Binomial model has a Beta distribution as 
\begin{equation}\label{truebeta}
p_j\vert \boldsymbol{y} \sim \textrm{Beta}(t_{1j}, t_{0j}),~\textrm{for}~ j=1, 2, \ldots, k,
\end{equation}
and each random effect of the Poisson model has a Gamma distribution as
\begin{equation}\label{truebeta_poisson}
\lambda_j\vert \boldsymbol{y} \sim \textrm{Gamma}(s_{1j}, s_{0j}),~\textrm{for}~ j=1, 2, \ldots, k.
\end{equation}

The unconditional posterior mean and variance of each random effect for the Binomial model specified in Equation \ref{postmean} and \ref{approximate_var} and those for the Poisson model in Equation \ref{postmean_poisson} and \ref{approximate_var_poisson}  are functions of the unconditional posterior moments of shrinkage factors and expected random effect. Once we plug-in the estimated unconditional posterior moments of shrinkage factors in Equation \ref{shrinkagemoments} and those of expected random effect in Equation \ref{priormeanmoments}, we get the estimates of the unconditional posterior mean and variance of each random effect in Equation \ref{postmean}  and \ref{postvar} denoted by $\hat{\mu}_{p_j}$ and $\hat{\sigma}^2_{p_j}$, respectively. For the Poisson model, let  $\hat{\mu}_{\lambda_j}$ and $\hat{\sigma}^2_{\lambda_j}$ denote the estimates of the unconditional posterior mean and variance in Equation \ref{postmean_poisson} and \ref{approximate_var_poisson}. The estimates of two parameters $t_{1j}$ and $t_{0j}$ in Equation \ref{truebeta} come as follows;
\begin{equation}\label{posttrueprob}
\hat{t}_{1j}=\bigg(\frac{\hat{\mu}_{p_j}(1-\hat{\mu}_{p_j})}{\hat{\sigma}^2_{p_j}}-1\bigg)\hat{\mu}_{p_j},~\textrm{and}~~\hat{t}_{0j}=\bigg(\frac{\hat{\mu}_{p_j}(1-\hat{\mu}_{p_j})}{\hat{\sigma}^2_{p_j}}-1\bigg)(1-\hat{\mu}_{p_j}).
\end{equation}
The estimates of two parameters $s_{1j}$ and $s_{0j}$ in Equation \ref{truebeta_poisson} are
\begin{equation}\label{posttrueprob}
\hat{s}_{1j}=\frac{\hat{\mu}_{\lambda_j}^2}{\hat{\sigma}^2_{\lambda_j}},~\textrm{and}~~\hat{s}_{0j}=\frac{\hat{\mu}_{\lambda_j}}{\hat{\sigma}^2_{\lambda_j}}.
\end{equation}


Finally, the approximate unconditional posterior distribution of random effect $j$ for the Binomial model is
\begin{equation}\label{trueapprox}
p_j\vert \boldsymbol{y} \stackrel{.}{\sim} \textrm{Beta}(\hat{t}_{1j}, \hat{t}_{0j}),
\end{equation}
and that for the Poisson model is
\begin{equation}\label{trueapprox_poisson}
\lambda_j\vert \boldsymbol{y} \stackrel{.}{\sim} \textrm{Gamma}(\hat{s}_{1j}, \hat{s}_{0j}).
\end{equation}

The ADM approximation is accurate if the true distribution of $p_j$ is close to the the Beta distribution \citep{morris1997, tang2011, morris2012}.

We use the posterior mean and (2.5\%, 97.5\%) quantiles (if we assign 95\% confidence level) of the posterior distribution in Equation \ref{trueapprox} for the Binomial model or in Equation \ref{trueapprox_poisson} for the Poisson model as the point and interval estimates of the random effect.

%The histogram on the right in Fig \ref{fig2} shows the ADM approximation to the 20,000 posterior samples of the true probability of the first player $p_1$ obtained via grid-sampling. In this case, the  approximation by the Beta distribution was not complete enough to catch the right-skewness of posterior distribution, the effect called the regression towards the mean. See section \ref{baseballex} for detailed analysis.



%\subsection[ADM]{The adjustment for density maximization}\label{ADM}





%\subsection[Posterior moment estimation]{Posterior Moment Estimation}

%Using the posterior mean and variance estimates of $B_{j}$, \pkg{Rgbp} estimates the posterior mean and variance of the random effects given only the data ($\boldsymbol{y}$). Taking the Normal-Normal model as an example, it estimates $E(\mu_{j}\vert \boldsymbol{y})$ and $Var(\mu_{j}\vert \boldsymbol{y})$ by the following identities
%\begin{eqnarray}
%E(\mu_{j}\vert \boldsymbol{y}) & = & E(E(\mu_{j}\vert \boldsymbol{y}, \boldsymbol{\beta}, A)\vert \boldsymbol{y}), \\
%Var(\mu_{j}\vert \boldsymbol{y}) & = & E(Var(\mu_{j}\vert \boldsymbol{y}, \boldsymbol{\beta}, A)\vert \boldsymbol{y})+Var(E(\mu_{j}\vert \boldsymbol{y}, \boldsymbol{\beta}, A)\vert \boldsymbol{y}),
%\end{eqnarray}  
%where both $E(\mu_{j}\vert \boldsymbol{y}, \boldsymbol{\beta}, A)$ and $Var(\mu_{j}\vert \boldsymbol{y}, \boldsymbol{\beta}, A)$ are linear functions of the shrinkage factors as specified in (\ref{normalpost}). We then estimate $E(\mu_{j}\vert \boldsymbol{y})$ and $Var(\mu_{j}\vert \boldsymbol{y})$ by plugging-in the approximately estimated two posterior moments of shrinkage factors.

%\subsection[Approximation to posterior distribution by moment matching]{Approximation to Posterior Distribution via Matching Moments}
%After estimating the two posterior moments, for example $E(\mu_{j}\vert \boldsymbol{y})$ and $Var(\mu_{j}\vert \boldsymbol{y})$, \code{gbp} approximates a posterior distribution of the mean effects given the data by assuming a reasonable distribution and matching moments. For the Normal-Normal model we actually estimate the first three moments and approximate $\pi(\mu_{j}\vert \boldsymbol{y})$ with a skewed-Normal distribution \citep{sn2013}. For the Poisson-Gamma model we approximate $\pi(\lambda_{j}\vert \boldsymbol{y})$ with a Gamma distribution and finally for the Binomial-Beta model we approximate the posterior distribution of $p_{j}$, $\pi(p_{j}\vert \boldsymbol{y})$, with a Beta distribution.


\section[Estimation]{The acceptance-rejection method for the Binomial model}\label{sec_accept}
As for the Binomial model, the package \pkg{Rgbp} also provides a way to  independently draw exact posterior samples of random effects and hyper-parameters via the acceptance-rejection method. We continue working on a logarithmic scale of $r$, $\alpha=\log(1/r)=-\log(r)$. The joint posterior density function of $\alpha$ and $\boldsymbol{\beta}$ based on their joint hyper-prior density function in Equation \ref{eq:hyper} is 
\begin{equation}
f(\alpha, \boldsymbol{\beta} \vert \boldsymbol{y})\propto f(\alpha, \boldsymbol{\beta})L(\alpha, \boldsymbol{\beta})\propto e^{\alpha}L(\alpha, \boldsymbol{\beta})d\alpha d\boldsymbol{\beta}.
\end{equation}

The Acceptance-Rejection (A-R) method \citep{everson2000inference, tang2002fitting} is useful when it is difficult to sample a parameter of interest $\theta$ directly from its target probability density $f(\theta)$, which is known up to a normalizing constant, but an easy-to-sample envelope function $g(\theta)$ is available. The A-R method samples $\theta$  from the envelope $g(\theta)$ and accepts it with a probability $\frac{f(\theta)}{Mg(\theta)}$, where $M$ is a constant making $f(\theta)/g(\theta)\le M$ for all $\theta$. The distribution of the accepted $\theta$ exactly follows $f(\theta)$. The A-R method is stable as long as the tails of the envelop function are thicker than those of the target density function. 

The goal of the A-R method  for the  Binomial model is to independently draw  posterior samples of hyper-parameters from $f(\alpha, \boldsymbol{\beta}\vert \boldsymbol{y})$, using an easy-to-sample envelop function $g(\alpha, \boldsymbol{\beta})$ that has thicker tails than the target density function. 

We factor the envelope function into two parts, $g(\alpha, \boldsymbol{\beta})=g_1(\alpha)g_2(\boldsymbol{\beta})$ to model the tails of each function separately. We consider the tail behavior of the conditional posterior density function $f(\alpha \vert \boldsymbol{\beta},  \boldsymbol{y})$ to come up with $g_1(\alpha)$; $f(\alpha \vert \boldsymbol{\beta},  \boldsymbol{y})$ behaves as $e^{-\alpha(k-1)}$ when $\alpha$ goes to $\infty$ and as $e^{\alpha}$ when $\alpha$ goes to $-\infty$. It indicates that $f(\alpha \vert \boldsymbol{\beta},  \boldsymbol{y})$ is skewed to the left because the right tail touches the $x$-axis faster than the left tail does it as long as $k>1$.  A skewed $t$-distribution  is a good candidate for $g_1(\alpha)$ because it behaves as a power law on both tails, leading to thicker tails than those of $f(\alpha \vert \boldsymbol{\beta},  \boldsymbol{y})$. 

It is too complicated to figure out the tail behaviors of $f(\boldsymbol{\beta} \vert \alpha,  \boldsymbol{y})$. However, since $f(\boldsymbol{\beta} \vert \alpha,  \boldsymbol{y})$ of the approximate Gaussian counterpart has a multivariate Gaussian density function \citep{tang2011, kelly2014advances}, we consider a multivariate $t$-distribution with 4  degrees of freedom as a good candidate for $g_2(\boldsymbol{\beta})$.


Specifically, we assume
\begin{eqnarray}\label{env}
g_1(\alpha) &=& g_1(\alpha; \mu, \sigma, a, b)~\equiv~\textrm{Skewed-}t(\alpha\vert\mu, \sigma, a, b),\\
g_2(\boldsymbol{\beta}) &=& g_2(\boldsymbol{\beta}; \boldsymbol{\mu}^\ast, S_{(m\times m)})~\equiv~t_{4}(\boldsymbol{\beta}\vert \boldsymbol{\mu}^\ast, S),\label{g2approx}
\end{eqnarray}
where the notation Skewed-$t(\alpha\vert\mu, \sigma, a, b)$ represents a density function of a skewed $t$-distribution  at $\alpha$ with location $\mu$, scale $\sigma$, degree of freedom $a+b$, and skewness $a-b$ for any positive constants $a$ and $b$ \citep{jones2003skew}. The article of \cite{jones2003skew} derives the mode of $g_1(\alpha)$ as
\begin{equation}\label{mode}
\mu+\frac{(a-b)\sqrt{a+b}}{\sqrt{(2a+1)(2b+1)}}.
\end{equation} 
It shows that the tails follow a power law as $\alpha^{-(2a+1)}$ on the left and $\alpha^{-(2b+1)}$ on the right when $b>a$. It also provides a representation to generate the random variable following their skewed-$t$ distribution as
\begin{equation}
\alpha\sim \mu+\sigma\frac{\sqrt{a+b}(2T-1)}{2\sqrt{T(1-T)}}, ~\textrm{where}~ T\sim \textrm{Beta}(a, b).
\end{equation}
The notation $t_{4}(\boldsymbol{\beta}\vert\boldsymbol{\mu}^\ast, S)$ in Equation \ref{g2approx} indicates a density function of a multivariate $t$-distribution at $\boldsymbol{\beta}$ with 4 degrees of freedom, a location vector $\boldsymbol{\mu}^\ast$, and a $m\times m$ scale matrix $S$ that leads to the variance-covariance matrix $2S$. 

We set the parameters of $g_1(\alpha)$ and $g_2(\boldsymbol{\beta})$, i.e., $\mu$, $\sigma$, $a$, $b$, $\boldsymbol{\mu}^\ast$, and $S$, to make the product of $g_1(\alpha)$ and $g_2(\boldsymbol{\beta})$ similar to the target joint posterior density $f(\alpha, \boldsymbol{\beta} \vert  \boldsymbol{y})$. First, we obtain the  mode of $f(\alpha, \boldsymbol{\beta} \vert  \boldsymbol{y})$ and the inverse of the  negative Hessian matrix at the modes, $-H^{-1}$. Let $(\hat{\alpha}, \hat{\boldsymbol{\beta}})$ denote the modes of $f(\alpha, \boldsymbol{\beta} \vert  \boldsymbol{y})$, $-H^{-1}_{\hat{\alpha}}$ indicate (1, 1) element of $-H^{-1}$, and $-H^{-1}_{\hat{\boldsymbol{\beta}}}$ represent  $-H^{-1}$ without the first row and  column. 
%(1, 1)  and $(2:(m+1), 2:(m+1))$ elements of the  inverse of the negative Hessian matrix at the modes, respectively.
% $f(\alpha, \boldsymbol{\beta} \vert \boldsymbol{y})\propto e^\alpha L(\alpha, \boldsymbol{\beta})d\alpha d\boldsymbol{\beta}$. 

Next, we set ($a,~ b$) to $(k,~ 2k)$ if $k<10$ (or otherwise $(\log(k),~ 2\log(k))$)   to maintain a left-skewness of $g_1(\alpha)$ and to keep  $a$ and $b$ small enough for thick tails. We match the mode of $g_1(\alpha)$ specified in Equation \ref{mode} to $\hat{\alpha}$ by fixing the location parameter $\mu$ at $\hat{\alpha}-(a-b)\sqrt{a+b}/\sqrt{(2a+1)(2b+1)}$. We set  the scale parameter $\sigma$ to $(-H^{-1}_{\hat{\alpha}})^{0.5}\eta$, where $\eta$ is a tuning parameter; $\eta=1.3$ is the default. When the A-R method produces extreme values of weights defined in Equation \ref{weight} below, we increase the value of $\eta$.  %To summarize, we use
%\begin{equation}
%g_1(\alpha)\equiv\textrm{Skewed-}t(\alpha\vert\mu=\hat{\alpha}+\zeta, \sigma=(-H^{-1}_{\hat{\alpha}})^{0.5}\eta, a=\log(k), b=2\log(k)),
%\end{equation}
%where $\zeta$ is a mode correction term defined as $-(a-b)\sqrt{a+b}/\sqrt{(2a+1)(2b+1)}$. 
%It is desirable to have a larger $\delta$ when $c$ in the class of hyper-prior distributions is closer to 0 because a smaller $c$ makes a thicker left tail of $\alpha$ a priori.
%, considering an asymptotic normality of the target density as $k$ increases. 
%In case of using the hyper-prior distribution $dr/r^{1.5}$, whose right tail is thicker than $dr/r^2$ a priori, we set a larger scale $\phi=5*\sqrt{-H^{-1}_{\hat{\alpha}}}$ to cover its thicker tail. The multiplication constants were chosen after simulation studies. We fix the skewness parameter $\delta$ at -2 because in many cases, combined with a large scale parameter, it helped the skewed-normal density cover a long left-tail of the marginal distribution of $\alpha$, known to have a left skewness \citep{morris2012shrinkage}. 

As for $g_2(\boldsymbol{\beta})$, we matches the location vector $\boldsymbol{\mu}^\ast$ to the mode $\hat{\boldsymbol{\beta}}$ and the scale matrix $S$ to $-H^{-1}_{\hat{\boldsymbol{\beta}}}/2$ so that the variance-covariance matrix becomes $-H^{-1}_{\hat{\boldsymbol{\beta}}}$;
\begin{equation}
g_2(\boldsymbol{\beta})\equiv t_4(\boldsymbol{\beta}\vert \boldsymbol{\mu}^\ast=\hat{\boldsymbol{\beta}}, S=-H^{-1}_{\hat{\boldsymbol{\beta}}}/2).
\end{equation}
%; when the sample size was small, it increased the acceptance rate of the A-R method


For the  implementation of the acceptance-rejection method, we obtain four times more trial samples than the desired number of samples $N$ independently from $g_1(\alpha)$ and $g_2(\boldsymbol{\beta})$. We calculate $4N$ weights, each of which is defined as
\begin{equation}\label{weight}
w_i\equiv w(\alpha^{(i)}, \boldsymbol{\beta}^{(i)})=\frac{f(\alpha^{(i)}, \boldsymbol{\beta}^{(i)}\vert \boldsymbol{y})}{g_1(\alpha^{(i)})g_2( \boldsymbol{\beta}^{(i)})}, ~\textrm{for}~ i=1, 2, \ldots, 4N.
\end{equation}
We accept each pair of $(\alpha^{(i)}, \boldsymbol{\beta}^{(i)})$ with a probability $w_i/M$ where $M$ is set to the maximum of all the $4N$ weights. The usual acceptance rates from  our data examples are around 25\%. In a case where we accept more than the desired number of samples $N$, we discard the redundant. If the number of accepted samples is smaller than $N$, then we sample additional pairs (6 times more than the shortage) and calculate a new maximum $M'$ from all the previous and new weights, accepting or rejecting the entire pairs again with new probabilities $w_i/M'$.




%Using the A-R method, for example, we fit the  Bayesian BBLR with $drd\beta_1/r^2$ on the data of 18 baseball players with $\boldsymbol{x}^T_j\boldsymbol{\beta}=\beta_1$ \citep{efron1975data}. To obtain 5,000 posterior samples of $(\alpha, \beta_1)$, we sample 20,000 trial pairs and make sure that there are not extremely large weights via the histogram of the 20,000 weights in the first plot of Figure \ref{fig1}. The method accepts 5,504 pairs out of 20,000 (27.52\%) based on the probabilities $w_i/M$ for $i=1, 2, \ldots, 20,000$.  The histograms of posterior samples of  $\alpha$ and $\beta_1$ and their scatter plot follow in Figure \ref{fig1}. For a comparison, we draw red density curves  representing 20,000 posterior samples of $(\alpha, \beta_1)$ obtained via a two-dimensional grid sampling method \citep{gelman2013bayesian}. The red contour lines in the scatter plot whose levels are $(0.05, 0.25, 0.50, 0.75, 0.95)$  indicate the target joint posterior density $\pi_{hyp.post}(\alpha, \beta_1\vert \boldsymbol{y})$.   The whole procedure of the A-R method takes less than a second to produce these posterior samples.

%\begin{figure}[h!]
%\includegraphics[scale = 0.4]{fig1_1.png}
%\caption[]{The result of fitting the Bayesian BBLR with $drd\boldsymbol{\beta}/r^2$  via the A-R method on the data of 18 baseball players  without any covariates. We display histograms of 20,000 weights (1st plot), the histograms of the posterior samples of $\alpha$ (2nd) and $\beta_1$ (3rd), and their scatter plot (4th).  The red density curves in the histograms of $\alpha$ and $\beta_1$ represent 20,000 posterior samples obtained via a grid sampling method. The red contour lines whose levels are (0.05, 0.25, 0.50, 0.75, 0.95) in the scatter plot indicate the joint posterior density $f(\alpha, \beta_1\vert\boldsymbol{y})$. }
%\label{fig1}
%\end{figure}


Once we have posterior samples of hyper-parameters, it is easy to obtain posterior samples of random effects via a Monte Carlo integration below.
\begin{equation}\label{mcintegration}
f(\boldsymbol{p}\vert \boldsymbol{y})=\int  f(\boldsymbol{p}\vert \alpha, \boldsymbol{\beta}, \boldsymbol{y})\cdot f(\alpha, \boldsymbol{\beta}\vert \boldsymbol{y})d\alpha d\boldsymbol{\beta}.
\end{equation}
The integration can be done by sampling $(p_1, p_2, \ldots, p_k)$ from the independent Beta conditional posterior distributions $f(p_j\vert \boldsymbol{\beta}, r, \boldsymbol{y})$ in Equation \ref{betapost}  given $r~(=e^{-\alpha})$ and $\boldsymbol{\beta}$ already sampled from $f(\alpha, \boldsymbol{\beta}\vert \boldsymbol{y})$ via the A-R method. 




\section{Frequency method checking}\label{sec4}
Whether the 95\%  interval estimates of random effects obtained by a specific model achieve the nominal 95\% confidence level  for any true parameter values is one of the key model evaluation criteria. A frequency method checking is a procedure to evaluate it, which is different from a model checking that tests whether a two-level model is appropriate for data (overdispersion exists in data) \citep{dean1992testing, modelchecking1996}.   Conditioning that the two-level model is appropriate, the frequency method checking   generates pseudo-data sets given specific values of hyper-parameters (a parametric bootstrapping) and estimates unknown coverage probabilities based on these mock data sets.


From now on, the explanation will be based on the Normal-Normal model because the idea can be easily applied to the other two models.




\subsection{Pseudo-data generation}\label{data_generation}
Figure \ref{fig:pseudo} displays the process of generating pseudo-data sets. It is noted that  the conjugate prior distribution of each random effect in Equation \ref{normalprior} is completely determined by two hyper-parameters, $A$ and $\boldsymbol{\beta}$. Fixing these hyper-parameters at specific values, we generate  $N_{sim}$ sets of random effects from the conjugate prior distribution, i.e., \{{\boldmath $\mu$}$^{(i)},~i=1, \ldots, N_{sim}\}$, where the superscript $(i)$ indicates the $i$-th simulation. Next, using the distribution of observed data in Equation \ref{normalobs}, we generate $N_{sim}$ sets of observed data sets $\{\boldsymbol{y}^{(i)},~i=1, \ldots, N_{sim}\}$ given each {\boldmath$\mu$}$^{(i)}$. Note that we generate one observed data set per one set of random effects.

\begin{figure}[h]
\begin{center}
\includegraphics[width=6cm]{process.png}
\caption{Pseudo-data generating process}
\label{fig:pseudo}
\end{center}
\end{figure}



\subsection{Coverage probability estimation}
After fitting a Normal-Normal model on each simulated data set, we obtain interval estimates of random effects $\boldsymbol{\mu}$. Let $(\hat{\mu}^{(l)}_{j, ~low}, ~\hat{\mu}^{(l)}_{j, ~upp})$ represent the lower and upper bounds of the interval estimate of random effect $j$ based on the $i$-th simulated data set given a specific confidence level.  Let's define a coverage indicator of random effect $j$ on the $i$-th mock data set as 
\begin{equation}\label{coverage_indicator}
I_{A, \boldsymbol{\beta}}(\mu_j^{(i)}) = \left\{ \begin{array}{ll}
1, & \textrm{if $\mu_j^{(i)}\in(\hat{\mu}^{(i)}_{j, ~low}, ~\hat{\mu}^{(i)}_{j, ~upp})$}\\
0, & \textrm{otherwise}
\end{array} \right.
\end{equation}
We consider the coverage indicators as functions of $A$ and $\boldsymbol{\beta}$ because outcomes of indicators depend on the simulated random effects and mock data generated by these hyper-parameters. 

%Let's suppress the subscript $(r, \boldsymbol{\beta})$ from the coverage indicator for simplicity.


%B$^3$LM model to $N_{sim}$ pseudo-datasets in order to obtain $k\times N_{sim}$ interval estimates,  \{$(\hat{p}^{(l)}_{j, ~low}, ~\hat{p}^{(l)}_{j, ~upp}), ~j=1,\ldots, k,~ i=1, \ldots, N_{sim}$\}.  

%The simple and Rao-Blackwellized unbiased coverage estimators and their unbiased variance estimators for the coverage evaluation.  

\subsubsection{Simple unbiased coverage estimator.}
When the confidence level is 95\%, the proportion of 95\% interval estimates that contain random effect $j$ is an intuitive choice for  the coverage rate estimator of random effect $j$. This estimator  implicitly assumes that there exist $k$ unknown coverage probabilities of random effects, denoted by $C_{A, \boldsymbol{\beta}}(\mu_j)$ for $j=1, 2, \ldots, k$, depending on the values of the hyper-parameters that generate random effects and mock data sets. The coverage indicators for random effect $j$ in Equation \ref{coverage_indicator} follow an independent and identically distributed  Bernoulli distribution given the unknown coverage rate $C_{A, \boldsymbol{\beta}}(\mu_j)$. The sample mean of these coverage indicators is a simple unbiased coverage estimator for $C_{A, \boldsymbol{\beta}}(\mu_j)$.  %estimates the coverage probability via the sample mean of these indicator variables for each group $j$, 
\begin{equation}
\bar{I}_{A, \boldsymbol{\beta}}(\mu_j)= \frac{1}{N_{sim}}\sum_{i=1}^{N_{sim}}I_{A, \boldsymbol{\beta}}(\mu_j^{(i)}),~ j=1, 2, \ldots, k.
\end{equation}
Note that $\bar{I}_{A, \boldsymbol{\beta}}(\mu_j)$ averages over possible values of $\mu_{j}$ and $y_{j}$ generated by specific values of $A$ and $\boldsymbol{\beta}$. 

 %true coverage probability for the $j$th group depending on specific values of $(r, \boldsymbol{\beta})$. 
%For reference, even if we sample $L$ datasets per one parameter set, $L\cdot N_{sim}$ indicators $I^{(i, l)}_{j}, i = 1, \ldots, N_{sim}, l = 1, \ldots, L$, have still $iid$ Bernoulli($p_{cov, j}$) distribution.

The unbiased variance estimator of $Var(\bar{I}_{A, \boldsymbol{\beta}}(\mu_j))$ is 
\begin{equation}\label{svar}
\widehat{Var}(\bar{I}_{A, \boldsymbol{\beta}}(\mu_j))=\frac{1}{N_{sim}(N_{sim}-1)}\sum_{i=1}^{N_{sim}}(I_{A, \boldsymbol{\beta}}(\mu_j^{(i)})-\bar{I}_{A, \boldsymbol{\beta}}(\mu_j))^{2},~ j=1, 2, \ldots, k.
\end{equation}

\subsubsection{Rao-Blackwellized unbiased coverage estimator.}
The frequency method checking is computationally expensive in nature because it  fits a model on every mock data set. The situation deteriorates if the number of simulations or the size of data is large, or the estimation method is computationally demanding. \citet{morris1997} and \cite{tang2002fitting} used a Rao-Blackwellized (RB) unbiased coverage estimator for the unknown coverage rate of each random effects, which is more efficient than the simple indicator-based coverage estimator.  For $j=1, 2, \ldots, k$,
\begin{equation}\label{RB_theory}
C_{A, \boldsymbol{\beta}}(\mu_j)=E(\bar{I}_{A, \boldsymbol{\beta}}(\mu_j)\vert A, \boldsymbol{\beta})=E\bigg[\frac{1}{N_{sim}}\sum_{i=1}^{N_{sim}}E(I_{A, \boldsymbol{\beta}}(\mu_j^{(i)})\vert A, \boldsymbol{\beta}, \boldsymbol{y}^{(i)})\bigg\vert A, \boldsymbol{\beta}\bigg],
\end{equation}
where the sample mean of conditional expectations inside the outer expectation is the RB unbiased coverage estimator. To be specific,
\begin{eqnarray}\label{RB}
\bar{I}^{RB}_{A, \boldsymbol{\beta}}(\mu_j) &=& \frac{1}{N_{sim}}\sum_{i=1}^{N_{sim}}E(I_{A, \boldsymbol{\beta}}(\mu_j^{(i)})\vert A, \boldsymbol{\beta}, \boldsymbol{y}^{(i)})\\
&=& \frac{1}{N_{sim}}\sum_{i=1}^{N_{sim}} Pr(\mu_j^{(i)}\in(\hat{\mu}^{(i)}_{j, ~low}, ~\hat{\mu}^{(i)}_{j, ~upp})\vert A, \boldsymbol{\beta},  \boldsymbol{y}^{(i)}).\nonumber
\end{eqnarray}
We  can easily compute the above conditional posterior probabilities using the cumulative density function of the Normal conditional posterior distribution of each random effect in Equation \ref{normalpost}. The variance of  $\bar{I}^{RB}_{A, \boldsymbol{\beta}}(\mu_j)$ is smaller than or equal to the variance of a simple coverage estimator $\bar{I}_{A, \boldsymbol{\beta}}(\mu_j)$ \citep{radhakrishna1945information, blackwell1947conditional}.




%, i.e., $Pr(p_j^{(l)}<\hat{p}^{(l)}_{j, ~upp}\vert r, \boldsymbol{\beta}, \boldsymbol{y}^{(l)})-Pr(p_j^{(l)}<\hat{p}^{(l)}_{j, ~low},\vert r, \boldsymbol{\beta}, \boldsymbol{y}^{(l)})$.


If one dataset $\boldsymbol{y}^{(l)}$ is simulated per one set of random effects $\boldsymbol{\mu}^{(l)}$, the variance estimator below is an unbiased estimator of $Var(\bar{I}^{RB}_{A, \boldsymbol{\beta}}(\mu_j) )$. For $j=1, 2, \ldots, k,$
\begin{equation}\label{RBvar}
\widehat{Var}(\bar{I}^{RB}_{A, \boldsymbol{\beta}}(\mu_j) )\equiv\frac{1}{N_{sim}(N_{sim}-1)}\sum_{i=1}^{N_{sim}}\bigg(E(I_{A, \boldsymbol{\beta}}(\mu_j^{(i)})\vert A, \boldsymbol{\beta}, \boldsymbol{y}^{(i)})-\bar{I}^{RB}_{A, \boldsymbol{\beta}}(\mu_j) \bigg)^{2}.
\end{equation}



\section[packageoverview]{Usage of functions in \pkg{Rgbp}}\label{sec5}
In this section, we describe the usage of the two main functions of \pkg{Rgbp}, i.e., \code{gbp} for model fitting and \code{coverage} for frequency method checking.

The basic usage of fitting a Normal-Normal model via the function \code{gbp} is simply
\begin{CodeChunk}
\begin{CodeInput}
R> output <- gbp(y, se, model = "gaussian")
\end{CodeInput}
\end{CodeChunk}

The argument \code{y} is a vector of $k$ observed sufficient statistics, the argument  \code{se} for the Normal model is a vector of $k$ standard errors of the corresponding sufficient statistics, and the argument \code{model} indicates that  \code{gbp} fits a Normal-Normal model.

There are many optional arguments for \code{gbp}.

\section[Examples]{Examples}\label{sec6}
\subsection[Known Second-level Mean]{Data of 31 hospitals with a known second-level mean}
\label{sec:ex:hosp}
In this example we adopt the perspective of a person living in the state of New York (NY) who has been suffering from severe coronary heart disease. If this person must receive coronary artery bypass graft (CABG) surgery soon, he or she might want to find the most reliable hospital for such a procedure.


For this purpose, data were gathered from 31 hospitals in NY composed of the number of deaths ($\textbf{z}_{(31\times1)}$) for a specified period after CABG surgeries and the total number of patients ($\textbf{n}_{(31\times1)}$) receiving CABG surgeries in each hospital. For reference, caseloads ($n_{j}$) can be interpreted as exposures. These data can be loaded into \proglang{R} using the following code where the symbol `\code{R>}' represents a command prompt and is not to be typed into \proglang{R}.

\begin{CodeChunk}
\begin{CodeInput}
R> z <- c( 3,   2,   5,  11,   9,  12,  12,   4,  10,  13,  14,   7,  12,
          11,  13,  22,  15,  11,  14,  11,  16,  14,   9,  15,  13,  35,
          26,  25,  20,  35,  27)
R> n <- c(67,  68, 210, 256, 269, 274, 278, 295, 347, 349, 358, 396, 431,
         441, 477, 484, 494, 501, 505, 540, 563, 593, 602, 629, 636, 729,
         849, 914, 940, 1193, 1340)
\end{CodeInput}
\end{CodeChunk}
or
\begin{CodeChunk}
\begin{CodeInput}
R> data(``hospital'')
R> z <- hospital$d
R> n <- hospital$n
\end{CodeInput}
\end{CodeChunk}


In addition, suppose one knows that the state-level death rate per exposure of this surgery is 0.030 ($\lambda_{0}$). Using these data and the known second-level mean ($\lambda_{0}$), \pkg{Rgbp} provides point and interval estimates of the true death rate ($\lambda_{j}$) so that one can evaluate each hospital's reliability. 


The independent Poisson distribution, i.e.,  $z_{j}\vert \lambda_{j}\stackrel{ind}{\sim} \textrm{Poisson}(n_{j}\lambda_{j})$, $j=1, \ldots, 31,$ would be an ideal choice to describe these data based on the Poisson approximation when $\lambda$ is small and $n$ is relatively large. 


Next, \code{gbp} fits the Poisson hierarchical model with the Gamma conjugate prior distribution as the NY state-level population distribution of the death rates whose mean is 0.030 ($\lambda_{0}=0.030$). For reference, the number of regression coefficients ($m$) is 0 because we do not need to estimate the prior mean (the second-level mean) via regression for this Poisson-Gamma model. 
\begin{CodeChunk}
\begin{CodeInput}
R> p <- gbp(z, n, mean.PriorDist = 0.03, model = "poisson")
R> p
\end{CodeInput}
\begin{CodeOutput}
Summary for each unit (sorted by n):

         obs.mean    n prior.mean shrinkage low.intv post.mean upp.intv post.sd
1          0.0448   67       0.03     0.911   0.0199    0.0313   0.0454 0.00653
2          0.0294   68       0.03     0.910   0.0189    0.0299   0.0435 0.00631
3          0.0238  210       0.03     0.765   0.0185    0.0285   0.0407 0.00566
4          0.0430  256       0.03     0.728   0.0225    0.0335   0.0467 0.00619
5          0.0335  269       0.03     0.718   0.0208    0.0310   0.0432 0.00573
6          0.0438  274       0.03     0.714   0.0229    0.0339   0.0472 0.00621
7          0.0432  278       0.03     0.711   0.0228    0.0338   0.0469 0.00617
8          0.0136  295       0.03     0.699   0.0157    0.0250   0.0366 0.00534
9          0.0288  347       0.03     0.663   0.0200    0.0296   0.0410 0.00536
10         0.0372  349       0.03     0.662   0.0222    0.0325   0.0446 0.00571
11         0.0391  358       0.03     0.656   0.0228    0.0331   0.0454 0.00579
12         0.0177  396       0.03     0.633   0.0165    0.0255   0.0363 0.00506
13         0.0278  431       0.03     0.613   0.0200    0.0292   0.0400 0.00511
14         0.0249  441       0.03     0.608   0.0191    0.0280   0.0387 0.00502
15         0.0273  477       0.03     0.589   0.0199    0.0289   0.0394 0.00499
16         0.0455  484       0.03     0.585   0.0256    0.0364   0.0491 0.00601
17         0.0304  494       0.03     0.580   0.0211    0.0302   0.0409 0.00506
18         0.0220  501       0.03     0.577   0.0180    0.0266   0.0369 0.00483
19         0.0277  505       0.03     0.575   0.0202    0.0290   0.0395 0.00494
20         0.0204  540       0.03     0.559   0.0173    0.0258   0.0358 0.00474
21         0.0284  563       0.03     0.548   0.0206    0.0293   0.0395 0.00485
22         0.0236  593       0.03     0.535   0.0187    0.0270   0.0369 0.00466
23         0.0150  602       0.03     0.532   0.0147    0.0230   0.0329 0.00466
24         0.0238  629       0.03     0.521   0.0188    0.0271   0.0368 0.00460
25         0.0204  636       0.03     0.518   0.0173    0.0254   0.0351 0.00455
26         0.0480  729       0.03     0.484   0.0286    0.0393   0.0516 0.00587
27         0.0306  849       0.03     0.446   0.0223    0.0303   0.0397 0.00445
28         0.0274  914       0.03     0.428   0.0208    0.0285   0.0374 0.00423
29         0.0213  940       0.03     0.421   0.0176    0.0249   0.0335 0.00407
30         0.0293 1193       0.03     0.364   0.0223    0.0296   0.0379 0.00397
31         0.0201 1340       0.03     0.338   0.0170    0.0235   0.0310 0.00360
colMeans           517       0.03     0.600   0.0201    0.0293   0.0403 0.00517
\end{CodeOutput}
\end{CodeChunk}

For reference, we need to type `\code{R> print(p, sort = FALSE)}' instead of `\code{R> p}' in order to list hospitals by the order of data input in the above output. `\code{R> p}' automatically sorts the output by the increasing order of caseload ($n_{j}$), as shown above.


The output contains information about observed death rates ($y_{j}$), caseloads ($n_{j}$), known prior mean ($\lambda_{0}$), shrinkage estimates ($\hat{B}_{j}$), lower bounds of interval estimates ($\hat{\lambda}_{j, low}$, 2.5\% percentiles of the approximate posterior distributions if 95\% confidence level is given), approximate posterior means ($\hat{\lambda}_{j}=E(\lambda_{j}\vert \boldsymbol{y})$), upper bounds  of interval estimates ($\hat{\lambda}_{j, upp}$, 97.5\% percentiles of the approximate posterior distributions), and standard deviations of the approximate posterior distributions ($sd(\lambda_{j}\vert \boldsymbol{y})$).


As we can see in (\ref{gammapost}), the posterior mean, $(1-B_{j})y_{j} + B_{j}\lambda_{0}$, is a convex combination of the sample mean and prior mean ($\lambda_{0}=0.030$) with the shrinkage factor, $B_{j}\equiv r / (r + n_{j})$, determining the weight. This makes intuitive sense because $r$ and $n_{j}$ can be interpreted as the degree (sample sizes) of prior and observed information respectively. If the second level has more information than the first level, i.e., ensemble sample size $r$ exceeds individual sample size $n_{j}$, then the estimator will shrink towards the prior mean more than 50\%. This is clear because, as caseload increases, shrinkage decreases, depending less on the NY state-level (second-level) information.


A function \code{summary} shows selective information on hospitals and more detailed estimation results, as below. To be specific, it displays some hospitals (not all as above) with minimum, median, and maximum caseloads ($n_{j}$). On top of that, more specific estimation results, such as the posterior mode and standard deviation of $\alpha\equiv\log(1/r)$, follow. \begin{CodeChunk}
\begin{CodeInput}
R> summary(p)
\end{CodeInput}
\begin{CodeOutput}
Main summary:

                    obs.mean    n prior.mean shrinkage low.intv post.mean
Unit with min(n)      0.0448   67       0.03     0.911   0.0199    0.0313   
Unit with median(n)   0.0455  484       0.03     0.585   0.0256    0.0364   
Unit with max(n)      0.0201 1340       0.03     0.338   0.0170    0.0235   
Overall Mean                  517       0.03     0.600   0.0201    0.0293   

                    upp.intv  post.sd
                      0.0454  0.00653
                      0.0491  0.00601
                      0.0310  0.00360
                      0.0403  0.00517

Second-level Variance Component Estimation Summary:
alpha = log(A) for Gaussian or alpha = log(1/r) for Binomial and Poisson data:

  post.mode.alpha post.sd.alpha post.mode.r
1           -6.53         0.576         684
\end{CodeOutput}
\end{CodeChunk}
The output of \code{summary} also provides $\hat{r}=\textrm{exp}(6.53)=684$, which is an indicator of how valuable and informative the hypothetical second-level hierarchy is. It means that observed sample means of hospitals whose caseloads are less than 684 will shrink toward the prior mean (0.030) more than 50\%. For example, the shrinkage estimate of the first hospital ($\hat{B}_{1}= 0.911$) was calculated by 684 / (684 + 67), where 67 is its caseload ($n_{1}$), and its posterior mean is $(1-0.911)*0.0448 + 0.911 * 0.030=0.0313$. As for this hospital, using more information from the prior distribution is an appropriate choice because the observed amount of information (67) is far less than the amount of state-level information (684).


To obtain a graphical summary the function \code{plot} can be used, as seen in Figure \ref{fig:hospshr}.

\begin{CodeChunk}
\begin{CodeInput}
R> plot(p)
\end{CodeInput}
\end{CodeChunk}
\begin{figure}[h]
\begin{center}
\includegraphics[width = 3.2in]{hospital1.png}
\caption{Shrinkage plot and 95\% interval plot for 31 hospitals}
\label{fig:hospshr}
\end{center}
\end{figure}

In Figure \ref{fig:hospshr} the regression towards the mean (RTTM) is obvious in the first graph; the observed sample means, empty dots on the upper horizontal line, are shrinking towards the known second-level mean (a blue vertical line at 0.030) to the different extents. Note that some hospitals' ranks have changed by shrinking much harder towards 0.030 than others. For example, the empty square symbol at the crossing point of the two left-most lines (8th and 23rd hospitals on the list above) indicates that  the seemingly safest hospital among 31 hospitals in terms of the observed death rate is probably not the safest in terms of the estimated posterior means. 


Intuitively, switching ranks for these two hospitals makes sense. To be specific, their observed death rates ($y_{j}$, $j=8, 23$) are 0.0136 and 0.0150 and caseloads ($n_{j}$, $j=8, 23$) are 295 and 602 each. Considering solely the observed death rates may lead to an unfair comparison because the latter hospital handled twice the caseload. \pkg{Rgbp} accounts for this caseload difference, making the death rate estimate for the former hospital shrink toward the state-level mean ($\lambda_{0}$=0.030) much harder than that for the latter hospital.


Note that the point estimates are not enough to evaluate hospital reliability because one hospital may have a lower point estimate but  bigger uncertainty (variance) than the other. In the second plot of Figure \ref{fig:hospshr}, the estimated 95\% intervals are displayed. We see that each posterior mean (red dot) is between the sample mean (empty dot) and the second-level mean (a blue horizontal line). For reference, we could plot this 95\% interval plot by the order of data input via \code{plot(p, sort = FALSE)}.

This 95\% interval plot reveals that the 31st hospital has the lowest upper bound even though its point estimate ($\hat{\lambda}_{31}=0.0235$) is slightly bigger than that of the 23rd hospital ($\hat{\lambda}_{23}=0.0230$). The observed death rates for these two hospitals ($y_{j}, j=23, 31$) are 0.0150 and 0.0201 and the caseloads ($n_{j}, j =23, 31$) are 602 and 1340 each. The 31st hospital has twice the caseload, which leads to borrowing less information from the NY state-level hierarchy (or shrinking less toward the state-level mean, 0.030) with smaller variance. Based on the point and interval estimates, the 31st hospital seems the most reliable one among all candidates. 


When fitting a model it is always a good idea to question how reliable the estimation procedure is. For example, does our procedure generate interval estimates that have good repeated sampling properties? To answer this question the \code{coverage} function generates pseudo-datasets assuming the estimated $r~(=683.53)$ is a true value. For reference, we can designate any other value of $r$, for example $r=600$, by adding another argument, \code{A.or.r = 600}, into the code below; \code{R> pcv <- coverage(p, A.or.r = 600, nsim = 1000)}.


In addition, \code{gbp} also provides interval estimates with different confidence levels, for example 90\%. For this, we need to go back to the code for fitting the model, adding another argument, \code{Alpha = 0.9}; \code{R> p <- gbp(z, n, Alpha = 0.9, mean.PriorDist = 0.03, model = "poisson")}.  Then, the code below will evaluate whether interval estimates achieve the 90\% confidence level.

\begin{CodeChunk}
\begin{CodeInput}
R> pcv <- coverage(p, nsim = 1000)
\end{CodeInput}
\end{CodeChunk}
\begin{figure}[h] 
\begin{center}
\includegraphics[width = 3in]{hospital2.png}
\caption{Coverage plot via frequency method checking for 31 hospitals}
\label{fig:hospitalcoverage}
\end{center}
\end{figure}

In Figure \ref{fig:hospitalcoverage}, the black horizontal line at 0.95 represents the nominal confidence level and the red circles indicate Rao-Blackwellized (RB) unbiased coverage estimates for 31 hospitals. The overall RB unbiased coverage estimate across all the hospitals is 0.953. And none of RB unbiased coverage estimates for 31 hospitals are less than 0.95 regardless of their caseloads ($n_{j}$). This result shows that the interval estimates for this particular dataset accurately achieves 95\% confidence under repeated sampling. Note that these estimates depend on the given true value of $r$, the known prior mean, and the assumption that the model is true.

The following code provides 31 RB unbiased coverage estimates for each hospital.
\begin{CodeChunk}
\begin{CodeInput}
R> pcv$coverageRB
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.955 0.954 0.955 0.954 0.954 0.954 0.954 0.954 0.954 0.954 0.954 0.954
[13] 0.954 0.953 0.953 0.954 0.953 0.954 0.954 0.953 0.953 0.953 0.954 0.953
[25] 0.953 0.953 0.952 0.953 0.952 0.952 0.952
\end{CodeOutput}
\end{CodeChunk}

And the code below shows 31 simple unbiased coverage estimates for each hospital.
\begin{CodeChunk}
\begin{CodeInput}
R> pcv$coverageS
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.949 0.960 0.958 0.958 0.950 0.945 0.960 0.953 0.960 0.956 0.955 0.946 
[13] 0.955 0.954 0.960 0.965 0.955 0.952 0.960 0.956 0.959 0.955 0.964 0.960 
[25] 0.945 0.942 0.947 0.960 0.940 0.946 0.956
\end{CodeOutput}
\end{CodeChunk}

The function \code{coverage} also calculates the standard errors for each hospital's RB unbiased coverage estimate defined in (\ref{RBvar}). The following code provides 31 standard errors for RB estimates.
\begin{CodeChunk}
\begin{CodeInput}
R> pcv$se.coverageRB
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.0016 0.0016 0.0014 0.0014 0.0013 0.0013 0.0013 0.0013 0.0012 0.0013 0.0012 
[12] 0.0012 0.0011 0.0011 0.00110.0011 0.0011 0.0011 0.0011 0.0011 0.0010 0.0010 
[23] 0.0010 0.0010 0.0010 0.0009 0.0009 0.0008 0.0008 0.0007 0.0007
\end{CodeOutput}
\end{CodeChunk}
Similarly, 31 standard errors for each simple unbiased coverage estimate defined in (\ref{svar}) are
\begin{CodeChunk}
\begin{CodeInput}
R> pcv$se.coverageS
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.0070 0.0062 0.0063 0.0063 0.0069 0.0072 0.0062 0.0067 0.0062 0.0065 0.0066
[12] 0.0072 0.0066 0.0066 0.0062 0.0058 0.0066 0.0068 0.0062 0.0065 0.0063 0.0066 
[23] 0.0059 0.0062 0.0072 0.0074 0.0071 0.0062 0.0075 0.0072 0.0065
\end{CodeOutput}
\end{CodeChunk}

Taking the first hospital as an example, the variance estimate of RB unbiased coverage estimate is about 19 times smaller than that of simple one. It means that 1,000 RB unbiased coverage estimates are as precise as 19,000 simple unbiased coverage estimates in terms of estimating true coverage probability for the first hospital, $p_{cov, 1}$.

For reference, two $31 \times 1,000$ matrices \code{raw.resultRB} and \code{raw.resultS}, each row of which is about each hospital, in \code{pcv} contain all the individual estimates, $I^{(i)}_{j}$ and $E(I^{(i)}_{j}\vert y^{(i)}_{j}, A, \boldsymbol{\beta})$.

\cite{1995} also investigated a similar ranking problem in hierarchical modeling, taking shrinkage into account.


\subsection[Unknown Second-level Mean and No Covariate]{Data of 8 schools with unknown second-level mean with no covariates} \label{sec:ex:8schools}

The Education Testing Service (ETS) conducted randomized experiments in eight separate schools (groups) to test whether students (units) SAT scores are effected by coaching. The dataset contains the estimated coaching effects on SAT scores ($y_{j}, j=1, \ldots, 8$) and standard errors ($se_{j}, j=1, \ldots, 8$) of the eight schools \citep{1981}.
\begin{CodeChunk}
\begin{CodeInput}
R> y  <- c(12, -3, 28,  7,  1,  8, 18, -1)
R> se <- c(18, 16, 15, 11, 11, 10, 10,  9)
\end{CodeInput}
\end{CodeChunk}
or
\begin{CodeChunk}
\begin{CodeInput}
R> data(``schools'')
R> y  <- schools$y
R> se <- schools$se
\end{CodeInput}
\end{CodeChunk}



Due to the nature of the test each school's coaching effect has an approximately Normal sampling distribution with known sampling variance, i.e., standard error of each school is assumed to be known. At the second hierarchy, the mean for each school is assumed to be drawn from a common Normal distribution and hence, we can use the Gaussian component of \code{gbp} to fit this Normal-Normal hierarchical model.


\begin{CodeChunk}
\begin{CodeInput}
R> g <- gbp(y, se, model = "gaussian")
R> g
\end{CodeInput}
\begin{CodeOutput}
Summary for each unit (sorted by se):

         obs.mean   se prior.mean shrinkage low.intv post.mean upp.intv post.sd
5           -1.00  9.0      8.168     0.408  -13.297     2.737   16.692   7.634
2            8.00 10.0      8.168     0.459   -7.255     8.077   23.361   7.810
7           18.00 10.0      8.168     0.459   -1.289    13.484   30.821   8.176
4            7.00 11.0      8.168     0.507   -8.780     7.592   23.602   8.257
6            1.00 11.0      8.168     0.507  -13.027     4.633   20.131   8.441
1           28.00 15.0      8.168     0.657   -2.315    14.979   38.763  10.560
3           -3.00 16.0      8.168     0.685  -17.130     4.650   22.477  10.096
8           12.00 18.0      8.168     0.734  -10.208     9.189   29.939  10.227
colMeans          12.5      8.168     0.552   -9.163     8.168   25.723   8.900
\end{CodeOutput}
\end{CodeChunk}
This output from \code{gbp} summarizes the results. In this Normal-Normal hierarchical model the amount of shrinkage for each unit is governed by the shrinkage factor, $B_j = V_j/(V_j + A)$. As such, schools whose variation within the school ($V_{j}$) is less than the between school variation ($A$) will shrink greater than $50\%$. The results provided by \code{gpb} suggests that there is little evidence that the training provided much added benefit due to the fact that every school's $95\%$ posterior interval contains 0. In the case where the number of groups is large \pkg{Rgbp} provides a summary feature:

\begin{CodeChunk}
\begin{CodeInput}
R> summary(g)
\end{CodeInput}
\begin{CodeOutput}
Main summary:

                      obs.mean   se prior.mean shrinkage low.intv post.mean
Unit with min(se)        -1.00  9.0       8.17     0.408   -13.30      2.74     
Unit with median(se)1     1.00 11.0       8.17     0.507   -13.03      4.63     
Unit with median(se)2     7.00 11.0       8.17     0.507    -8.78      7.59     
Unit with max(se)        12.00 18.0       8.17     0.734   -10.21      9.19     
Overall Mean                   12.5       8.17     0.552    -9.16      8.17     

                      upp.intv post.sd
                          16.7    7.63
                          20.1    8.44
                          23.6    8.26
                          29.9   10.23
                          25.7    8.90

Second-level Variance Component Estimation Summary:
alpha = log(A) for Gaussian or alpha =  log(1/r) for Binomial and Poisson data:

  post.mode.alpha post.sd.alpha post.mode.A
1            4.77          1.14         118


Regression Summary:

      estimate   se z.val p.val
beta0    8.168 5.73 1.425 0.154
\end{CodeOutput}
\end{CodeChunk}
The summary provides results regarding the second level hierarchy parameters. It can be seen that the estimate of the second level mean, \code{beta0}, is not significantly different from 0 suggesting that there was no effect of the coaching program on SAT math scores. 


\pkg{Rgbp} also provides functionality to plot the results of the analysis as seen in Figure \ref{fig:8schoolsplot}. Plotting the results provides a visual aid to understanding but is only largely beneficial when the number of groups $(k)$ is small.

\begin{CodeChunk}
\begin{CodeInput}
R> plot(g)
\end{CodeInput}
\end{CodeChunk}

\begin{figure}[h] 
\begin{center}
\includegraphics[width = 3.2in]{school1.png}
\caption{Shrinkage plot and 95\% interval plot for 8 schools}
\label{fig:8schoolsplot}
\end{center}
\end{figure}


The frequency method checking, assuming the model is correct, generates new pseudo-data from our assumed model. Unless otherwise specified, the procedure fixes the hyper-parameter values at their estimates ($\hat{A}$ and $\hat{\boldsymbol{\beta}}_0$ in this example) and then simulates ``true'' $\theta_j$ for each group $j$. The model is then estimated and this is repeated an \code{nsim} number of times to estimate the coverage probabilities of the procedure.  

\begin{CodeChunk}
\begin{CodeInput}
R> gcv <- coverage(g, nsim = 1000)
\end{CodeInput}
\end{CodeChunk}
\begin{figure}[h] 
\begin{center}
\includegraphics[width = 3in]{school2.png}
\caption{Coverage plot via frequency method checking for 8 schools}
\label{fig:schoolcoverage}
\end{center}
\end{figure}

As seen in Figure \ref{fig:schoolcoverage} the desired $95\%$ confidence (black horizontal line at 0.95) is achieved (actually, exceeded) for each school in this example. Note that all the coverage estimates depend on the chosen true values of $A$ and $\beta_{0}$, and the assumption that the model is valid.


In addition, Rao-Blackwellized (RB) unbiased coverage estimate and its standard error for each school can be gotten with the command below.
\begin{CodeChunk}
\begin{CodeInput}
R> gcv$coverageRB
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.966 0.959 0.967 0.960 0.959 0.962 0.960 0.966
\end{CodeOutput}
\end{CodeChunk}
\begin{CodeChunk}
\begin{CodeInput}
R> gcv$se.coverageRB
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.0013 0.0012 0.0013 0.0013 0.0011 0.0011 0.0010 0.0017
\end{CodeOutput}
\end{CodeChunk}

All the individual RB coverage estimates are saved in the $8\times1,000$ matrix, \code{gcv$raw.resultRB}, each row of which is about each school.


% For reference, it took 188 seconds for 10,000 simulations  on one of authors' MacBook Pro with a 2.3 GHz dual-core Intel i5 CPU. In MCMC, the exact simulation would be too time-consuming. One could compare curve in Figure \ref{fig:schoolcoverage} with a curve from MCMC.

%\begin{CodeChunk}
%\begin{CodeInput}
%R> shr <- seq(0.01, 0.99, length.out = 20)
%R> A.trial <- mean(n) * (1 - shr) / shr
%R> cov.save <- sapply (1 : length(A.trial), function(i){
%R>               coverage(g, A.or.r = A.trial[i], reg.coef = 8.168, 
%R>                        nsim = 500)$average.coverageRB
%R>             })
%\end{CodeInput}
%\end{CodeChunk}
%\begin{figure}[h]
%\begin{center}
%\includegraphics[width = 5.5cm]{school3.png}
%\end{center}
%\end{figure}


\subsection[Unknown Second-level Mean and One Covariate]{Data of 18 baseball players with unknown second-level mean and one covariate} 

The following dataset from the New York Times published on 26 April 1970 contains information on the batting averages and positions (outfielder=1,otherwise=0) of 18 major league baseball players through their first 45 official at-bats of the 1970 season \citep{1975}.

\begin{CodeChunk}
\begin{CodeInput}
R> z <- c(18, 17, 16, 15, 14, 14, 13, 12, 11, 11, 10, 10, 10, 10, 10,  9,  8,  7)
R> n <- c(45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45)
R> x <- c( 1,  1,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  1,  1,  0,  0,  0) 
\end{CodeInput}
\end{CodeChunk}
or
\begin{CodeChunk}
\begin{CodeInput}
R> data(``baseball'')
R> z <- baseball$Hits
R> n <- baseball$At.Bats
R> x <- ifelse(baseball$Position == "fielder", 1, 0)
\end{CodeInput}
\end{CodeChunk}
%$
Conditioning on the true batting average for each player we assume that the at-bats are independent and therefore, $z_{j}\vert p_{j}\stackrel{ind}{\sim} \textrm{Binomial}(45, p_{j}), ~j=1, \ldots, 18$. Our goal is to obtain point and interval estimates of the true batting average, $p_{j}$, for each player, whilst considering the additional information on whether the player is an outfielder or not. \code{gbp} provides a way to incorporate such covariate information seamlessly into the second-level hierarchy such that information is shared and regression towards the mean (RTTM) occurs within outfielders and non-outfielders separately. %For reference, we can also assume the Normal distribution using $y_{j}=z_{j} / n_{j}$ and $V_{j}=\bar{y}(1-\bar{y})/n$, where $\bar{y}=\sum_{j} z_{j} / \sum_{j} n_{j}$, $j=1,\ldots, 18$. 


\begin{CodeChunk}
\begin{CodeInput}
R> b <- gbp(z, n, x, model = "binomial")
R> b
\end{CodeInput}
\begin{CodeOutput}
Summary for each unit (sorted by n):

         obs.mean  n   X1 prior.mean shrinkage low.intv post.mean upp.intv post.sd
1           0.400 45 1.00      0.310     0.715    0.248     0.335    0.429  0.0462
2           0.378 45 1.00      0.310     0.715    0.244     0.329    0.420  0.0448
3           0.356 45 1.00      0.310     0.715    0.240     0.323    0.411  0.0437
4           0.333 45 1.00      0.310     0.715    0.236     0.316    0.403  0.0429
5           0.311 45 1.00      0.310     0.715    0.230     0.310    0.396  0.0424
6           0.311 45 0.00      0.233     0.715    0.179     0.256    0.341  0.0415
7           0.289 45 0.00      0.233     0.715    0.175     0.249    0.331  0.0400
8           0.267 45 0.00      0.233     0.715    0.171     0.243    0.323  0.0388
9           0.244 45 0.00      0.233     0.715    0.166     0.237    0.315  0.0380
10          0.244 45 1.00      0.310     0.715    0.210     0.291    0.379  0.0432
11          0.222 45 0.00      0.233     0.715    0.161     0.230    0.308  0.0377
12          0.222 45 0.00      0.233     0.715    0.161     0.230    0.308  0.0377
13          0.222 45 0.00      0.233     0.715    0.161     0.230    0.308  0.0377
14          0.222 45 1.00      0.310     0.715    0.202     0.285    0.375  0.0441
15          0.222 45 1.00      0.310     0.715    0.202     0.285    0.375  0.0441
16          0.200 45 0.00      0.233     0.715    0.155     0.224    0.302  0.0377
17          0.178 45 0.00      0.233     0.715    0.148     0.218    0.297  0.0381
18          0.156 45 0.00      0.233     0.715    0.140     0.211    0.292  0.0389
colMeans          45 0.44      0.267     0.715    0.191     0.267    0.351  0.0410
\end{CodeOutput}
\end{CodeChunk}
% Our model reflects on the additional information, \emph{i.e.}, indicator covariate (1 for outfielder and 0 for other positions), estimating two different prior means, 0.310 and 0.233. Also
Note that the shrinkage estimates are the same for all players due to the fact that they are determined solely by the relative amount of information between the first-level and the second-level hierarchies, ($\hat{B}_{j}\equiv \hat{r} / (\hat{r}+45)$ = $113 / (113+45)$ = 0.715). 


\begin{CodeChunk}
\begin{CodeInput}
R> summary(b)
\end{CodeInput}
\begin{CodeOutput}
Main summary:

                            obs.mean  n    X1 prior.mean shrinkage low.intv 
Unit with min(obs.mean)        0.156 45 0.000      0.233     0.715    0.140     
Unit with median(obs.mean)1    0.244 45 0.000      0.233     0.715    0.166     
Unit with median(obs.mean)2    0.244 45 1.000      0.310     0.715    0.210     
Unit with max(obs.mean)        0.400 45 1.000      0.310     0.715    0.248     
Overall Mean                         45 0.444      0.267     0.715    0.191     


                            post.mean upp.intv post.sd
                                0.211    0.292  0.0389
                                0.237    0.315  0.0380
                                0.291    0.379  0.0432
                                0.335    0.429  0.0462
                                0.267    0.351  0.0410

Second-level Variance Component Estimation Summary:
alpha = log(A) for Gaussian or alpha =  log(1/r) for Binomial and Poisson data:

  post.mode.alpha post.sd.alpha post.mode.r
1           -4.73         0.957         113


Regression Summary:

      estimate    se  z.val p.val
beta0   -1.194 0.131 -9.129 0.000
beta1    0.389 0.187  2.074 0.038
\end{CodeOutput}
\end{CodeChunk}

From the \code{Regression Summary}, one of the outputs of \code{summary}, we see that the two prior means distinguishing outfielders from other positions are significantly different (p-value for $\hat{\beta}_1$= 0.038). Also, the positive sign of $\hat{\beta}_{1}$ indicates that the population mean batting average for all outfielders tends to be higher than that for those in the other positions (estimated odds ratio = exp(0.389)=1.48).

\begin{CodeChunk}
\begin{CodeInput}
R> plot(b)
\end{CodeInput}
\end{CodeChunk}
\begin{figure}[h]
\begin{center}
\includegraphics[width = 3.2in]{baseball1.png}
\caption{Shrinkage plot and 95\% interval plot for 18 baseball players}
\label{fig:baseball}
\end{center}
\end{figure}

It is evident in the shrinkage plot in Figure \ref{fig:baseball} that shrinkage occurs from the sample means (empty dots) on the upper horizontal line towards the two prior means, 0.233 and 0.310. For reference, the short red line symbols on dots are for when two or more points have the same mean and are plotted over each other. For example, five players (from the 11th player to the 15th) have the same sample mean (0.222) and at this point on the upper horizontal line, there are short red lines toward five directions.

%For reference, the observed sample mean among outfielders is 0.308 and that among other positions is 0.231. But why are the estimated two prior means from this Binomial model 0.310 and 0.233 each? This can be attributed to the logistic regression which estimates prior mean by a non-linear logit function of $x'\beta$, causing a small bias (see 3.3). The Normal model can avoid this small bias because it uses a linear regression to estimate these prior means.


The 95\% interval plot shows the range of true batting average for each player, which clarifies the regression toward the mean (RTTM) within two groups. The 10th, 14th, and 15th players, for example, are outfielders but their observed batting averages are far lower than the first five outfielders. This can likely be attributed to their bad luck because their observed batting averages are close to the lower bounds of their interval estimates. RTTM suggests that their batting averages will shrink towards the expected prior mean of outfielders (0.310) in the long run.


As in the previous examples in Section \ref{sec:ex:hosp} and \ref{sec:ex:8schools}, in order to check the level of trust in these interval estimates, we can proceed to frequency method checking by assuming the estimates, 112.95 for $\hat{r}$ and (-1.194, ~0.389) for $\hat{\boldsymbol{\beta}}$, are given values. 

\begin{CodeChunk}
\begin{CodeInput}
R> bcv <- coverage(b, nsim = 1000) 
\end{CodeInput}
\end{CodeChunk}
\begin{figure}[h]
\begin{center}
\includegraphics[width = 3in]{baseball2.png}
\caption{Coverage plot via frequency method checking for 18 players}
\label{fig:baseball2}
\end{center}
\end{figure}

For reference, to do the frequency method checking at different true values, we need to specify additional arguments in the coverage function. For example, if we want to try different true values, either 100 for $r$ or (-1, 0.2) for $(\beta_{0}, \beta_{1})$, the additional arguments are \code{A.or.r = 100} and \code{reg.coef = c(-1, 0.2)}; \code{coverage(b, A.or.r = 100, reg.coef = c(-1, 0.2), nsim = 1000)}.


Finally, in Figure \ref{fig:baseball2}, we see that the overall Rao-Blackwellized unbiased coverage estimate is 0.972 (across all the players), conservatively satisfying the definition of the 95\% confidence interval. Note that each coverage estimate depends on given true values of $r$ and $\beta_{(2\times1)}$, and the assumption that the model is valid.


The Rao-Blackwellized unbiased coverage estimates and their standard errors for each player follow.
\begin{CodeChunk}
\begin{CodeInput}
R> bcv$coverageRB
\end{CodeInput}
%$
\begin{CodeOutput}
 [1] 0.971 0.973 0.972 0.972 0.970 0.973 0.973 0.974 0.973 0.973 0.971 0.973 
[13] 0.973 0.972 0.972 0.971 0.973 0.971
\end{CodeOutput}
\end{CodeChunk}
\begin{CodeChunk}
\begin{CodeInput}
R> bcv$se.coverageRB
\end{CodeInput}
\begin{CodeOutput}
 [1] 0.0015 0.0012 0.0013 0.0014 0.0016 0.0010 0.0012 0.0010 0.0010 0.0013 
[11] 0.0015 0.0013 0.0019 0.0013 0.0014 0.0015 0.0011 0.0014
\end{CodeOutput}
\end{CodeChunk}

%$

All the simulation results are saved in the $18\times1,000$ matrix, \code{bcv$raw.resultRB}, each row of which is for each player.

%$



\section[Discussion]{Discussion and summary} \label{discussion}
\pkg{Rgbp} is an \proglang{R} package for estimating and validating two-level Gaussian, Binomial and Poisson hierarchical models. The package aims to provide a procedure that is computationally efficient with good frequency properties and includes ``frequency method checking'' functionality to examine repeated sampling properties and to test that the method is valid at specified hyper-parameter values.

As an alternative to other maximization based estimation methods such as MLE and REML, \pkg{Rgbp} provides point and interval estimates of parameters via ADM. Using the ADM approach, with our specified choice of priors, protects from cases of overshrinkage and undercoverage from which the aforementioned methods suffer from \citep{accuracy1988}.


A benefit of \pkg{Rgbp} is that it produces non-random output and so results are easily reproduced and compared across studies. In addition to being a standalone analysis tool the package can be used as an aid in a broader estimation procedure. For example, by checking the similarity of output of \pkg{Rgbp} and that of another estimation procedure (such as MCMC) the package can be used as a confirmatory tool to check whether the alternative procedure has been programmed correctly. In addition, the parameter estimates obtained via \pkg{Rgbp} can be used to initialize a MCMC thus decreasing time to convergence. % Note, however, that this is only valid when the same model under both procedures is assumed. 

Due to its speed and ease of use, \pkg{Rgbp} can be used as a method of preliminary data analysis. Such results may tell statisticians and practitioners alike whether a more intensive method in terms of implementation and computational time, such as MCMC, is needed. 


In addition to the built in ``frequency method checking'' procedure the package can be used to undergo ``model checking''. For example, in the Gaussian hierarchical model, the assumed marginal distribution of the data is given in (\ref{normalmarginal}). By substituting the point estimates of $A$ and $\boldsymbol{\beta}$ from the package into this marginal distribution a test can be constructed to see whether the data follow the marginal distribution suggested by the hierarchical model.


\section[acknowledgments]{Acknowledgments}
The authors thank Professor Cindy Christiansen, Professor Phil Everson and the 2012 class of Harvard's Stat 324r: Parametric Statistical Inference and Modeling for their valuable inputs.

\appendix
\section{Unconditional posterior variance of the Binomial model}\label{apppostvar}

\section{Unconditional posterior variance of the Poisson model}\label{apppostvar_poisson}

\bibliography{bibliography}



\end{document}
